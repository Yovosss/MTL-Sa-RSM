{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Test Prediction Metrics for PreAttnMMs_LCPN\n",
    "        - do some necessary post-processing for PreAttnMMs_LCPN model results\n",
    "        - at last, we will get the target_labels and pred_labels of TEST dataset with shape [sample_num, 11]\n",
    "        - we will calculate some metrics based on the above target_labels and pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from addict import Dict\n",
    "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
    "                             classification_report, confusion_matrix, f1_score,\n",
    "                             hamming_loss, precision_recall_curve,\n",
    "                             precision_score, recall_score, roc_auc_score,\n",
    "                             roc_curve, zero_one_loss)\n",
    "from torch import nn\n",
    "\n",
    "from helper.data import DataPreprocess\n",
    "from helper.data_loader import data_loaders\n",
    "from helper.utils import get_config, load_checkpoint, set_seed\n",
    "from models.model import PreAttnMMs\n",
    "from train_modules.evaluation_metrics import evaluate4test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the model structure and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  Loading previously preprocessed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wzx/HC4FUOV2/models/grud_layer.py:120: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/data/wzx/HC4FUOV2/models/grud_layer.py:120: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "config_0 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_0\"))\n",
    "config_1 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_1\"))\n",
    "config_2 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_2\"))\n",
    "config_6 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_6\"))\n",
    "config_7 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_7\"))\n",
    "\n",
    "set_seed(seed=config_0.seed)\n",
    "\n",
    "# Load the preprocessed data\n",
    "dp = DataPreprocess(config_0)\n",
    "data, label, indices = dp.load()\n",
    "\n",
    "n_classes_0 = len(label['taxonomy'][config_0.experiment.local_task])\n",
    "n_classes_1 = len(label['taxonomy'][config_1.experiment.local_task])\n",
    "n_classes_2 = len(label['taxonomy'][config_2.experiment.local_task])\n",
    "n_classes_6 = len(label['taxonomy'][config_6.experiment.local_task])\n",
    "n_classes_7 = len(label['taxonomy'][config_7.experiment.local_task])\n",
    "\n",
    "model_0 = PreAttnMMs(config_0, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_0)\n",
    "model_0.to(config_0.train.device_setting.device)\n",
    "model_1 = PreAttnMMs(config_1, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_1)\n",
    "model_1.to(config_1.train.device_setting.device)\n",
    "model_2 = PreAttnMMs(config_2, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_2)\n",
    "model_2.to(config_2.train.device_setting.device)\n",
    "model_6 = PreAttnMMs(config_6, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_6)\n",
    "model_6.to(config_6.train.device_setting.device)\n",
    "model_7 = PreAttnMMs(config_7, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_7)\n",
    "model_7.to(config_7.train.device_setting.device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer_0 = torch.optim.Adam(\n",
    "    params = model_0.parameters(),\n",
    "    lr = config_0.train.optimizer.learning_rate,\n",
    "    weight_decay=config_0.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_1 = torch.optim.AdamW(\n",
    "    params = model_1.parameters(),\n",
    "    lr = config_1.train.optimizer.learning_rate,\n",
    "    weight_decay=config_1.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_2 = torch.optim.RMSprop(\n",
    "    params = model_2.parameters(),\n",
    "    lr = config_2.train.optimizer.learning_rate,\n",
    "    weight_decay=config_2.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_6 = torch.optim.RMSprop(\n",
    "    params = model_6.parameters(),\n",
    "    lr = config_6.train.optimizer.learning_rate,\n",
    "    weight_decay=config_6.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_7 = torch.optim.Adagrad(\n",
    "    params = model_7.parameters(),\n",
    "    lr = config_7.train.optimizer.learning_rate,\n",
    "    weight_decay=config_7.train.optimizer.weight_decay\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read the checkpoint file and load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial90).pt for node-0 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial65).pt for node-1 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial94).pt for node-2 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial77).pt for node-6 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial21).pt for node-7 task\n"
     ]
    }
   ],
   "source": [
    "checkpoint_base = config_0.train.checkpoint.dir\n",
    "checkpoint_dir_0 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-0', 'Standardization', 'auc')\n",
    "checkpoint_dir_1 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-1', 'Standardization', 'macro-auc')\n",
    "checkpoint_dir_2 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-2', 'Standardization', 'auc')\n",
    "checkpoint_dir_6 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-6', 'Standardization', 'auc')\n",
    "checkpoint_dir_7 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-7', 'Standardization', 'auc')\n",
    "\n",
    "# get the best checkpoint .pt file\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_0):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_0 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_0, 0))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_1):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_1 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_1, 1))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_2):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_2 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_2, 2))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_6):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_6 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_6, 6))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_7):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_7 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_7, 7))\n",
    "\n",
    "\n",
    "# reload the checkpoint file and run on test Dataset\n",
    "best_epoch_model_file_0 = os.path.join(checkpoint_dir_0, target_model_0)\n",
    "if os.path.isfile(best_epoch_model_file_0):\n",
    "    best_performance_0, config_0 = load_checkpoint(best_epoch_model_file_0, \n",
    "                                                model=model_0,\n",
    "                                                config=config_0,\n",
    "                                                optimizer=optimizer_0)\n",
    "best_epoch_model_file_1 = os.path.join(checkpoint_dir_1, target_model_1)\n",
    "if os.path.isfile(best_epoch_model_file_1):\n",
    "    best_performance_1, config_1 = load_checkpoint(best_epoch_model_file_1, \n",
    "                                                model=model_1,\n",
    "                                                config=config_1,\n",
    "                                                optimizer=optimizer_1)\n",
    "best_epoch_model_file_2 = os.path.join(checkpoint_dir_2, target_model_2)\n",
    "if os.path.isfile(best_epoch_model_file_2):\n",
    "    best_performance_2, config_2 = load_checkpoint(best_epoch_model_file_2, \n",
    "                                                model=model_2,\n",
    "                                                config=config_2,\n",
    "                                                optimizer=optimizer_2)\n",
    "best_epoch_model_file_6 = os.path.join(checkpoint_dir_6, target_model_6)\n",
    "if os.path.isfile(best_epoch_model_file_6):\n",
    "    best_performance_6, config_6 = load_checkpoint(best_epoch_model_file_6, \n",
    "                                                model=model_6,\n",
    "                                                config=config_6,\n",
    "                                                optimizer=optimizer_6)\n",
    "best_epoch_model_file_7 = os.path.join(checkpoint_dir_7, target_model_7)\n",
    "if os.path.isfile(best_epoch_model_file_7):\n",
    "    best_performance_7, config_7 = load_checkpoint(best_epoch_model_file_7, \n",
    "                                                model=model_7,\n",
    "                                                config=config_7,\n",
    "                                                optimizer=optimizer_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load the data and statistics of each local task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_0 = indices['folds_idx_with_txy']['parent-node-0'][0,:][2]\n",
    "indice_1 = indices['folds_idx_with_txy']['parent-node-1'][0,:][2]\n",
    "indice_2 = indices['folds_idx_with_txy']['parent-node-2'][0,:][2]\n",
    "indice_6 = indices['folds_idx_with_txy']['parent-node-6'][0,:][2]\n",
    "indice_7 = indices['folds_idx_with_txy']['parent-node-7'][0,:][2]\n",
    "sample_size = indice_0.shape[0]\n",
    "data_0 = {\n",
    "    'X_t': data['X_t'][indice_0], \n",
    "    'T_t': data['T_t_rel'][indice_0],\n",
    "    'X_t_mask': data['X_t_mask'][indice_0],\n",
    "    'deltaT_t': data['deltaT_t'][indice_0],\n",
    "    'X_val': data['static_data_val'][indice_0],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_0],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_0].tolist()\n",
    "}\n",
    "data_1 = {\n",
    "    'X_t': data['X_t'][indice_1], \n",
    "    'T_t': data['T_t_rel'][indice_1],\n",
    "    'X_t_mask': data['X_t_mask'][indice_1],\n",
    "    'deltaT_t': data['deltaT_t'][indice_1],\n",
    "    'X_val': data['static_data_val'][indice_1],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_1],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_1].tolist()\n",
    "}\n",
    "data_2 = {\n",
    "    'X_t': data['X_t'][indice_2], \n",
    "    'T_t': data['T_t_rel'][indice_2],\n",
    "    'X_t_mask': data['X_t_mask'][indice_2],\n",
    "    'deltaT_t': data['deltaT_t'][indice_2],\n",
    "    'X_val': data['static_data_val'][indice_2],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_2],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_2].tolist()\n",
    "}\n",
    "data_6 = {\n",
    "    'X_t': data['X_t'][indice_6], \n",
    "    'T_t': data['T_t_rel'][indice_6],\n",
    "    'X_t_mask': data['X_t_mask'][indice_6],\n",
    "    'deltaT_t': data['deltaT_t'][indice_6],\n",
    "    'X_val': data['static_data_val'][indice_6],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_6],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_6].tolist()\n",
    "}\n",
    "data_7 = {\n",
    "    'X_t': data['X_t'][indice_7], \n",
    "    'T_t': data['T_t_rel'][indice_7],\n",
    "    'X_t_mask': data['X_t_mask'][indice_7],\n",
    "    'deltaT_t': data['deltaT_t'][indice_7],\n",
    "    'X_val': data['static_data_val'][indice_7],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_7],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_7].tolist()\n",
    "}\n",
    "\n",
    "stat_0 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,3]\n",
    "}\n",
    "stat_1 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,3]\n",
    "}\n",
    "stat_2 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,3]\n",
    "}\n",
    "stat_6 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,3]\n",
    "}\n",
    "stat_7 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,3]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rescale_stdize(x, mean, std):\n",
    "    \"\"\"\n",
    "    standardize the non-time-series data and time-series data\n",
    "    :param x: A np.array witn shape (t_i, d)\n",
    "            mean: A np.array with shape (d,)\n",
    "            std: A np.array with shape (d,)\n",
    "    :return A np.array with same shape as x with rescaled values\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        return (x - mean) / std\n",
    "    elif x.ndim == 2:\n",
    "        return (x - mean[np.newaxis, :]) / std[np.newaxis, :]\n",
    "    elif x.ndim == 3:\n",
    "        return np.asarray([(xx - mean[np.newaxis, :]) / std[np.newaxis, :] for xx in x])\n",
    "\n",
    "def _fillnan(x, mean):\n",
    "        \"\"\"\n",
    "        fill the nan value in non-time-series data\n",
    "        :param x: A np.array of static variables with shape (d,)\n",
    "               mean: A np.array of mean value of each variable with shape (d,)\n",
    "        :return A np.array without nan value\n",
    "        \"\"\"\n",
    "        x[np.isnan(x)] = mean[np.isnan(x)]\n",
    "\n",
    "        return x  \n",
    "\n",
    "def _rescale_stdize(x, mean, std):\n",
    "    \"\"\"\n",
    "    standardize the non-time-series data and time-series data\n",
    "    :param x: A np.array witn shape (t_i, d)\n",
    "            mean: A np.array with shape (d,)\n",
    "            std: A np.array with shape (d,)\n",
    "    :return A np.array with same shape as x with rescaled values\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        return (x - mean) / std\n",
    "    elif x.ndim == 2:\n",
    "        return (x - mean[np.newaxis, :]) / std[np.newaxis, :]\n",
    "    elif x.ndim == 3:\n",
    "        return np.asarray([(xx - mean[np.newaxis, :]) / std[np.newaxis, :] for xx in x])\n",
    "\n",
    "def _locf_numpy(X, X_nan):\n",
    "    \"\"\"Numpy implementation of LOCF.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray,\n",
    "        Time series containing missing values (NaN) to be imputed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_imputed : array,\n",
    "        Imputed time series.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This implementation gets inspired by the question on StackOverflow:\n",
    "    https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "    \"\"\"\n",
    "    trans_X = X.transpose((1, 0))\n",
    "    trans_X_nan = X_nan.transpose((1, 0))\n",
    "    mask = np.isnan(trans_X_nan)\n",
    "    n_features, n_steps  = mask.shape\n",
    "    idx = np.where(~mask, np.arange(n_steps), 0)\n",
    "    np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "\n",
    "    X_imputed = trans_X[np.arange(n_features)[:, None], idx]\n",
    "    X_imputed = X_imputed.transpose((1, 0))\n",
    "\n",
    "    # If there are values still missing,\n",
    "    # they are missing at the beginning of the time-series sequence.\n",
    "    # Impute them with self.nan\n",
    "    if np.isnan(X_imputed).any():\n",
    "        X_imputed = np.nan_to_num(X_imputed, nan=0)\n",
    "\n",
    "    return X_imputed\n",
    "\n",
    "def f_empirical_mean(data, stat):\n",
    "\n",
    "    X_rescaled = _rescale_stdize(data['X_t'], stat['X_t_mean'], stat['X_t_std'])\n",
    "    X_rescaled = X_rescaled.reshape(-1, data['X_t'].shape[-1])\n",
    "    empirical_mean = np.nanmean(X_rescaled, axis=0)\n",
    "\n",
    "    return empirical_mean\n",
    "\n",
    "def _preprocess_sample(raw_sample, stat, empirical_mean):\n",
    "    \"\"\"\"\n",
    "    normalize the sample data\n",
    "    :param: raw_sample -> List[array(), array(), array(), array(), List[List[]]]\n",
    "    :return: Dict{'X_t': np.array([]),\n",
    "                    'X': np.array([]),\n",
    "                    'X_t_mask': np.array([]),\n",
    "                    'deltaT_t': np.array([]),\n",
    "                    'X_t_filledLOCF': np.array([]),\n",
    "                    'y_classes': List[List[int]],\n",
    "                    'empirical_mean': np.array([])}\n",
    "    \"\"\"\n",
    "    sample = {}\n",
    "\n",
    "    sample['X_t'] = _rescale_stdize(raw_sample[0], stat['X_t_mean'], stat['X_t_std'])\n",
    "    sample['X_t'] = np.nan_to_num(sample['X_t'])\n",
    "    \n",
    "    # fill the nan value in X_val and normalize X_val\n",
    "    raw_sample[4] = _fillnan(raw_sample[4], stat['X_val_mean'])\n",
    "    raw_sample[4] = _rescale_stdize(raw_sample[4], stat['X_val_mean'], stat['X_val_std'])\n",
    "    # concatenate the static variables\n",
    "    sample['X'] = np.concatenate((raw_sample[4], raw_sample[5]))\n",
    "\n",
    "    # forward fill nan value in np.array\n",
    "    sample['X_t_filledLOCF'] = _locf_numpy(sample['X_t'], raw_sample[0])\n",
    "\n",
    "    sample['empirical_mean'] = empirical_mean\n",
    "\n",
    "    sample['X_t_mask'] = raw_sample[2]\n",
    "    sample['deltaT_t'] = raw_sample[3] / 86400 # 24*60*60\n",
    "    sample['y_classes'] = raw_sample[-1]\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _localize_label(config, label, batch_labels):\n",
    "    \"\"\"\n",
    "    :param batch_labels: label idx of one batch, List[List[int]], e.g. [[0, 1, 3], [0, 2, 6, 9],...]\n",
    "    :return batch_local_label: np.array([int]), e.g. np.array([0, 1, ...])\n",
    "    \"\"\"\n",
    "    label_dict = {}\n",
    "    for idx, value in enumerate(label['taxonomy'][config.experiment.local_task]):\n",
    "        label_dict[value] = idx\n",
    "    \n",
    "    print(label_dict)\n",
    "    print(batch_labels)\n",
    "    \n",
    "    local_labels = []\n",
    "    for label in batch_labels:\n",
    "        for label_idx in label:\n",
    "            if label_idx in label_dict:\n",
    "                local_labels.append(label_dict[label_idx])\n",
    "                break\n",
    "\n",
    "    assert len(local_labels) == len(batch_labels), \"The labels are missed during localization, please recheck!\"\n",
    "\n",
    "    return local_labels\n",
    "\n",
    "def _check_input(batch):\n",
    "    for key, value in batch.items():\n",
    "        # convert the data type if in need\n",
    "        batch[key] = value.to(config_0.train.device_setting.device)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def collect(config, label, batch):\n",
    "    batch_X_t = []\n",
    "    batch_X_t_mask = []\n",
    "    batch_deltaT_t = []\n",
    "    batch_X_t_filledLOCF = []\n",
    "    batch_empirical_mean = []\n",
    "    batch_X = []\n",
    "    batch_label = []\n",
    "\n",
    "    # copy batch_size times of one sample\n",
    "    batch_copy = []\n",
    "    for i in range(64):\n",
    "        batch_copy.append(batch)\n",
    "\n",
    "    for sample in batch_copy:\n",
    "        batch_X_t.append(sample['X_t'])\n",
    "        batch_X_t_mask.append(sample['X_t_mask'])\n",
    "        batch_deltaT_t.append(sample['deltaT_t'])\n",
    "        batch_X_t_filledLOCF.append(sample['X_t_filledLOCF'])\n",
    "        batch_empirical_mean.append(sample['empirical_mean'])\n",
    "        batch_X.append(sample['X'])\n",
    "        batch_label.append(sample['y_classes'])\n",
    "\n",
    "    return {\n",
    "        'X': torch.tensor(np.array(batch_X)).to(torch.float32),\n",
    "        'X_t': torch.tensor(np.array(batch_X_t)).to(torch.float32),\n",
    "        'X_t_mask': torch.tensor(np.array(batch_X_t_mask)).to(torch.float32),\n",
    "        'deltaT_t': torch.tensor(np.array(batch_deltaT_t)).to(torch.float32),\n",
    "        'X_t_filledLOCF': torch.tensor(np.array(batch_X_t_filledLOCF)).to(torch.float32),\n",
    "        'empirical_mean': torch.tensor(np.array(batch_empirical_mean)).to(torch.float32)\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make hierarchical predictions based on the label hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_array = np.zeros((indice_0[:2112].shape[0], 11))\n",
    "predcit_labels_array = np.zeros((indice_0[:2112].shape[0], 11))\n",
    "\n",
    "target_labels = data_0['y_classes_unique']\n",
    "\n",
    "for index, sample_idx in enumerate(indice_0[:2112]):\n",
    "    raw_sample = [data_0[s][index] for s in ['X_t', 'T_t', 'X_t_mask', 'deltaT_t', 'X_val' , 'X_cat', 'y_classes_unique']]\n",
    "    \n",
    "    # parent-node-0\n",
    "    empirical_mean = f_empirical_mean(data_0, stat_0)\n",
    "    processed_sample = _preprocess_sample(raw_sample, stat_0, empirical_mean)\n",
    "    batch = collect(config_0, label, processed_sample)\n",
    "    \n",
    "    model_0.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = _check_input(batch)\n",
    "        logits = model_0(inputs)\n",
    "        predictions = F.softmax(logits, dim=1)\n",
    "        pred_label_0 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "        if pred_label_0 == 0:\n",
    "            predcit_labels_array[index, 0] = 1\n",
    "        elif pred_label_0 == 1:\n",
    "            predcit_labels_array[index, 1] = 1\n",
    "    \n",
    "    if pred_label_0 == 0:\n",
    "        # parent-node-1\n",
    "        empirical_mean = f_empirical_mean(data_1, stat_1)\n",
    "        processed_sample = _preprocess_sample(raw_sample, stat_1, empirical_mean)\n",
    "        batch = collect(config_1, label, processed_sample)\n",
    "\n",
    "        model_1.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = _check_input(batch)\n",
    "            logits = model_1(inputs)\n",
    "            predictions = F.softmax(logits, dim=1)\n",
    "            pred_label_1 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "            if pred_label_1 == 0:\n",
    "                predcit_labels_array[index, 2] = 1\n",
    "            elif pred_label_1 == 1:\n",
    "                predcit_labels_array[index, 3] = 1\n",
    "            elif pred_label_1 == 2:\n",
    "                predcit_labels_array[index, 4] = 1\n",
    "\n",
    "    elif pred_label_0 == 1:\n",
    "        # parent-node-2\n",
    "        empirical_mean = f_empirical_mean(data_2, stat_2)\n",
    "        processed_sample = _preprocess_sample(raw_sample, stat_2, empirical_mean)\n",
    "        batch = collect(config_2, label, processed_sample)\n",
    "\n",
    "        model_2.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = _check_input(batch)\n",
    "            logits = model_2(inputs)\n",
    "            predictions = F.softmax(logits, dim=1)\n",
    "            pred_label_2 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "            if pred_label_2 == 0:\n",
    "                predcit_labels_array[index, 5] = 1\n",
    "            elif pred_label_2 == 1:\n",
    "                predcit_labels_array[index, 6] = 1\n",
    "\n",
    "        if pred_label_2 == 0:\n",
    "            # parent-node-6\n",
    "            empirical_mean = f_empirical_mean(data_6, stat_6)\n",
    "            processed_sample = _preprocess_sample(raw_sample, stat_6, empirical_mean)\n",
    "            batch = collect(config_6, label, processed_sample)\n",
    "\n",
    "            model_6.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs = _check_input(batch)\n",
    "                logits = model_6(inputs)\n",
    "                predictions = F.softmax(logits, dim=1)\n",
    "                pred_label_6 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "                if pred_label_6 == 0:\n",
    "                    predcit_labels_array[index, 7] = 1\n",
    "                elif pred_label_6 == 1:\n",
    "                    predcit_labels_array[index, 8] = 1\n",
    "        \n",
    "        elif pred_label_2 == 1:\n",
    "            # parent-node-7\n",
    "            empirical_mean = f_empirical_mean(data_7, stat_7)\n",
    "            processed_sample = _preprocess_sample(raw_sample, stat_7, empirical_mean)\n",
    "            batch = collect(config_7, label, processed_sample)\n",
    "\n",
    "            model_7.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs = _check_input(batch)\n",
    "                logits = model_7(inputs)\n",
    "                predictions = F.softmax(logits, dim=1)\n",
    "                pred_label_7 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "                if pred_label_7 == 0:\n",
    "                    predcit_labels_array[index, 9] = 1\n",
    "                elif pred_label_7 == 1:\n",
    "                    predcit_labels_array[index, 10] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transform the target labels of TEST dataset into form of [sample_size, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _all_node_label_wo_root(batch_label):\n",
    "    \"\"\"\n",
    "    tranform y_classes_unique to all-node label without ROOT node\n",
    "    :params: batch_label, List[List[], List[], ...]-->[[0,1,3], [0,1,5], [0,2,6,8], ...]\n",
    "    :Return: all_node_labels without root node, List[List[], List[], ...]-->[[1,0,1,0,0,0,0,0,0,0,0], [1,0,0,0,1,0,0,0,0,0,0],...]\n",
    "    \"\"\"\n",
    "    all_node_labels = np.zeros((len(batch_label), 11))\n",
    "    for i, label in enumerate(batch_label):\n",
    "        for j in label[1:]:\n",
    "            all_node_labels[i][j-1] = 1\n",
    "\n",
    "    return all_node_labels\n",
    "\n",
    "target_labels_array = _all_node_label_wo_root(target_labels[:2112])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  本实验中，完全匹配样本数为：609，占比：0.2883522727272727 \n",
      "         非完全匹配样本数为：1503， 占比：0.7116477272727273 \n",
      "             不完整路径预测的样本数为：0， 占比：0.0 \n",
      "                 不完整路径预测中预测正确的样本数为：0， 占比：0 \n",
      "                 不完整路径预测中预测错误的样本数为：0，占比：0 \n",
      "             完整路径预测的样本数为：1503，占比：1.0 \n",
      "                 第一层预测错误的样本数为：415，占比：0.2761144377910845 \n",
      "                 第一层预测正确的样本数为：1088，占比：0.7238855622089155 \n",
      "                     第二层感染细分类预测错误的样本数为：906，占比：0.8327205882352942 \n",
      "                     第二层非感染细分类预测错误的样本数为：182，占比：0.16727941176470587 \n",
      "                         NIID和Neo层即分类预测错误的样本数为：125，占比：0.6868131868131868 \n",
      "                         NIID和Neo层预测正确，但第三层预测错误的样本数为：57，占比：0.3131868131868132 \n",
      "                     第二层及后续层预测存在违反类别约束错误的样本数为：0，占比：0.0\n",
      "INFO:  本实验中, 测试样本数量为: 2112, \n",
      "         完全匹配样本数为：609, 占比: 0.2883522727272727 \n",
      "         非完全匹配样本数为: 1503, 占比: 0.7116477272727273 \n",
      "             不完整路径预测的样本数为: 0, 占全部测试样本比例为: 0.0 \n",
      "                 不完整路径预测中预测正确的样本数为: 0, 占不完整路径预测样本的比例为: 0 \n",
      "             完整路径预测的样本数为: 1503, 占全部测试样本比例为: 0.7116477272727273 \n",
      "                 违反类别层次关系约束的样本数为: 0, 占全部测试样本比例为: 0.0 \n",
      "                 不违反类别层次关系约束的样本数为: 1503, 占全部测试样本比例为: 0.7116477272727273 \n",
      "                     其中预测标签与真实标签之间的LCA高度的均值为: 1.635395874916833\n",
      "The process finished!\n",
      "{'exact_match_ratio_top1': 0.8035037878787878, 'exact_match_ratio_top2': 0.3153409090909091, 'exact_match_ratio_overall': 0.2883522727272727, 'exact_match_ratio_layer1': 0.8035037878787878, 'exact_match_ratio_layer2': 0.3153409090909091, 'exact_match_ratio_layer3': 0.7173295454545454, 'exact_match_ratio_layer3.1': 0.13443830570902393, 'exact_match_ratio_layer_avg': 0.6120580808080808, 'exact_match_ratio_layer_avg1': 0.41776100089290696, 'accuracy': array([0.80350379, 0.80350379, 0.52651515, 0.52130682, 0.89772727,\n",
      "       0.82007576, 0.86505682, 0.92424242, 0.86647727, 0.921875  ,\n",
      "       0.91856061]), '01loss_top1': 0.19649621212121215, '01loss_top2': 0.6846590909090908, '01loss_overall': 0.7116477272727273, '01loss_layer1': 0.19649621212121215, '01loss_layer2': 0.6846590909090908, '01loss_layer3': 0.2826704545454546, '01loss_layer3.1': 0.8655616942909761, '01loss_layer_avg': 0.38794191919191917, '01loss_layer_avg1': 0.582238999107093, 'hamming_loss': 0.193741391184573, 'hamming_score': 0.4566761363636432, 'precision': array([0.83352601, 0.66753927, 0.58802817, 0.19148936, 0.03125   ,\n",
      "       0.20238095, 0.60769231, 0.        , 0.07936508, 0.42975207,\n",
      "       0.11111111]), 'macro-precision': 0.3401940296325241, 'micro-precision': 0.5288753799392097, 'precision_layer1': array([0.83352601, 0.66753927]), 'macro-precision_layer1': 0.7505326392882001, 'micro-precision_layer1': 0.8035037878787878, 'precision_layer2': array([0.58802817, 0.19148936, 0.03125   , 0.20238095, 0.60769231]), 'macro-precision_layer2': 0.3241681581578944, 'micro-precision_layer2': 0.3153409090909091, 'precision_layer3': array([0.        , 0.07936508, 0.42975207, 0.11111111]), 'macro-precision_layer3': 0.15505706414797327, 'micro-precision_layer3': 0.19109947643979058, 'precision_layer3.1': array([0.        , 0.12195122, 0.59090909, 0.33333333]), 'macro-precision_layer3.1': 0.26154841093865483, 'micro-precision_layer3.1': 0.28627450980392155, 'macro-precision_avg': 0.4099192871980226, 'macro-precision_avg1': 0.4454164027949164, 'micro-precision_avg': 0.43664805780316257, 'micro-precision_avg1': 0.46837306892453956, 'recall': array([0.91905672, 0.46961326, 0.30363636, 0.5308311 , 0.04166667,\n",
      "       0.22173913, 0.25239617, 0.        , 0.28571429, 0.35135135,\n",
      "       0.00606061]), 'macro-recall': 0.30746051389913664, 'micro-recall': 0.5110132158590308, 'recall_layer1': array([0.91905672, 0.46961326]), 'macro-recall_layer1': 0.6943349918482759, 'micro-recall_layer1': 0.8035037878787878, 'recall_layer2': array([0.30363636, 0.5308311 , 0.04166667, 0.22173913, 0.25239617]), 'macro-recall_layer2': 0.27005388521354173, 'micro-recall_layer2': 0.3153409090909091, 'recall_layer3': array([0.        , 0.28571429, 0.35135135, 0.00606061]), 'macro-recall_layer3': 0.1607815607815608, 'micro-recall_layer3': 0.13443830570902393, 'recall_layer3.1': array([0.        , 0.28571429, 0.35135135, 0.00606061]), 'macro-recall_layer3.1': 0.1607815607815608, 'micro-recall_layer3.1': 0.13443830570902393, 'macro-recall_avg': 0.3750568126144595, 'macro-recall_avg1': 0.3750568126144595, 'micro-recall_avg': 0.41776100089290696, 'micro-recall_avg1': 0.41776100089290696, 'f1': array([0.8742043 , 0.55135135, 0.40047962, 0.28144989, 0.03571429,\n",
      "       0.21161826, 0.35665914, 0.        , 0.1242236 , 0.3866171 ,\n",
      "       0.01149425]), 'macro-f1': 0.2939828914818921, 'micro-f1': 0.5197908887229277, 'f1_layer1': array([0.8742043 , 0.55135135]), 'macro-f1_layer1': 0.7127778278429991, 'micro-f1_layer1': 0.8035037878787878, 'f1_layer2': array([0.40047962, 0.28144989, 0.03571429, 0.21161826, 0.35665914]), 'macro-f1_layer2': 0.2571842389770065, 'micro-f1_layer2': 0.3153409090909091, 'f1_layer3': array([0.        , 0.1242236 , 0.3866171 , 0.01149425]), 'macro-f1_layer3': 0.1305837389324456, 'micro-f1_layer3': 0.15783783783783784, 'f1_layer3.1': array([0.        , 0.17094017, 0.44067797, 0.01190476]), 'macro-f1_layer3.1': 0.15588072473665693, 'micro-f1_layer3.1': 0.18295739348370926, 'macro-f1_avg': 0.36684860191748375, 'macro-f1_avg1': 0.3752809305188875, 'micro-f1_avg': 0.42556084493584495, 'micro-f1_avg1': 0.4339340301511354, 'hd_sensitivity_with_partial': 1.0, 'hd_sensitivity_wo_partial': 1.0, 'error': {'all_correct_sample': {'number': 609, 'ratio': 0.2883522727272727, 'index': [0, 2, 3, 7, 9, 11, 13, 18, 25, 30, 31, 32, 33, 40, 49, 55, 56, 57, 61, 67, 74, 76, 79, 82, 83, 87, 91, 102, 111, 112, 114, 116, 119, 123, 129, 130, 132, 139, 142, 147, 148, 150, 151, 156, 160, 161, 164, 166, 168, 174, 176, 179, 183, 187, 189, 192, 197, 198, 200, 203, 208, 214, 217, 218, 220, 224, 226, 227, 229, 230, 233, 239, 246, 249, 255, 256, 258, 260, 261, 263, 265, 266, 267, 268, 271, 272, 280, 290, 297, 298, 300, 301, 302, 305, 311, 317, 320, 321, 323, 324, 328, 332, 334, 335, 344, 349, 351, 353, 354, 363, 371, 376, 382, 384, 393, 396, 399, 407, 409, 413, 419, 426, 431, 436, 437, 440, 446, 453, 454, 456, 458, 464, 465, 468, 471, 473, 478, 481, 482, 486, 488, 489, 490, 495, 497, 501, 502, 503, 507, 516, 518, 520, 523, 533, 538, 539, 540, 548, 549, 553, 560, 564, 567, 568, 572, 576, 578, 583, 586, 587, 588, 591, 596, 599, 606, 615, 618, 619, 621, 629, 635, 636, 639, 649, 651, 655, 660, 661, 665, 668, 672, 679, 680, 681, 688, 699, 710, 712, 713, 714, 716, 718, 721, 727, 737, 739, 744, 749, 753, 756, 757, 762, 764, 766, 768, 774, 775, 781, 786, 792, 800, 801, 816, 817, 822, 828, 829, 832, 839, 844, 845, 849, 855, 856, 857, 858, 859, 860, 862, 866, 870, 873, 875, 882, 886, 894, 897, 900, 901, 905, 908, 912, 915, 917, 924, 926, 937, 939, 940, 943, 945, 955, 957, 959, 960, 964, 966, 968, 972, 976, 978, 982, 987, 989, 991, 995, 998, 1003, 1004, 1008, 1010, 1011, 1014, 1015, 1016, 1017, 1020, 1021, 1023, 1025, 1028, 1029, 1036, 1041, 1043, 1047, 1050, 1053, 1057, 1060, 1063, 1064, 1067, 1071, 1072, 1077, 1078, 1083, 1085, 1086, 1088, 1089, 1090, 1093, 1095, 1097, 1099, 1102, 1104, 1106, 1107, 1116, 1119, 1125, 1128, 1132, 1134, 1137, 1139, 1145, 1146, 1147, 1148, 1150, 1151, 1152, 1171, 1175, 1178, 1179, 1180, 1182, 1187, 1189, 1191, 1192, 1193, 1196, 1199, 1201, 1202, 1204, 1205, 1210, 1212, 1213, 1214, 1219, 1220, 1224, 1226, 1227, 1228, 1232, 1235, 1236, 1238, 1240, 1242, 1243, 1244, 1246, 1252, 1263, 1264, 1281, 1283, 1289, 1290, 1295, 1296, 1297, 1299, 1301, 1305, 1311, 1312, 1316, 1320, 1324, 1325, 1328, 1334, 1336, 1338, 1341, 1342, 1347, 1350, 1353, 1355, 1361, 1362, 1363, 1369, 1372, 1376, 1377, 1378, 1379, 1384, 1386, 1389, 1391, 1392, 1407, 1408, 1409, 1410, 1413, 1418, 1419, 1420, 1423, 1426, 1427, 1429, 1432, 1438, 1441, 1442, 1447, 1448, 1455, 1459, 1463, 1464, 1474, 1475, 1477, 1482, 1483, 1489, 1492, 1494, 1496, 1497, 1499, 1504, 1506, 1516, 1521, 1522, 1524, 1528, 1529, 1532, 1533, 1540, 1543, 1545, 1548, 1553, 1555, 1558, 1563, 1565, 1566, 1567, 1570, 1571, 1572, 1573, 1574, 1578, 1581, 1583, 1592, 1601, 1608, 1612, 1614, 1626, 1633, 1634, 1637, 1640, 1647, 1649, 1653, 1663, 1665, 1673, 1674, 1683, 1687, 1692, 1693, 1699, 1702, 1704, 1712, 1715, 1724, 1726, 1727, 1737, 1743, 1744, 1748, 1755, 1756, 1765, 1769, 1773, 1775, 1782, 1787, 1792, 1800, 1801, 1807, 1812, 1813, 1814, 1829, 1831, 1838, 1840, 1844, 1850, 1853, 1855, 1866, 1869, 1875, 1876, 1881, 1883, 1884, 1885, 1886, 1888, 1890, 1891, 1895, 1903, 1904, 1909, 1914, 1917, 1925, 1926, 1933, 1934, 1937, 1939, 1940, 1942, 1945, 1950, 1953, 1958, 1959, 1960, 1962, 1964, 1970, 1971, 1973, 1974, 1977, 1979, 1982, 1992, 1993, 1994, 1997, 1999, 2001, 2008, 2009, 2010, 2023, 2024, 2028, 2030, 2034, 2039, 2041, 2042, 2050, 2056, 2057, 2063, 2066, 2069, 2070, 2072, 2073, 2079, 2081, 2087, 2090, 2093, 2097, 2101, 2107, 2108]}, 'notall_correct_sample': {'number': 1503, 'ratio': 0.7116477272727273, 'index': [1, 4, 5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 58, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 77, 78, 80, 81, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 117, 118, 120, 121, 122, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 157, 158, 159, 162, 163, 165, 167, 169, 170, 171, 172, 173, 175, 177, 178, 180, 181, 182, 184, 185, 186, 188, 190, 191, 193, 194, 195, 196, 199, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 219, 221, 222, 223, 225, 228, 231, 232, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 257, 259, 262, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 299, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 322, 325, 326, 327, 329, 330, 331, 333, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 352, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 377, 378, 379, 380, 381, 383, 385, 386, 387, 388, 389, 390, 391, 392, 394, 395, 397, 398, 400, 401, 402, 403, 404, 405, 406, 408, 410, 411, 412, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 427, 428, 429, 430, 432, 433, 434, 435, 438, 439, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 455, 457, 459, 460, 461, 462, 463, 466, 467, 469, 470, 472, 474, 475, 476, 477, 479, 480, 483, 484, 485, 487, 491, 492, 493, 494, 496, 498, 499, 500, 504, 505, 506, 508, 509, 510, 511, 512, 513, 514, 515, 517, 519, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 541, 542, 543, 544, 545, 546, 547, 550, 551, 552, 554, 555, 556, 557, 558, 559, 561, 562, 563, 565, 566, 569, 570, 571, 573, 574, 575, 577, 579, 580, 581, 582, 584, 585, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 620, 622, 623, 624, 625, 626, 627, 628, 630, 631, 632, 633, 634, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647, 648, 650, 652, 653, 654, 656, 657, 658, 659, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 678, 682, 683, 684, 685, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 711, 715, 717, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 740, 741, 742, 743, 745, 746, 747, 748, 750, 751, 752, 754, 755, 758, 759, 760, 761, 763, 765, 767, 769, 770, 771, 772, 773, 776, 777, 778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 793, 794, 795, 796, 797, 798, 799, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 818, 819, 820, 821, 823, 824, 825, 826, 827, 830, 831, 833, 834, 835, 836, 837, 838, 840, 841, 842, 843, 846, 847, 848, 850, 851, 852, 853, 854, 861, 863, 864, 865, 867, 868, 869, 871, 872, 874, 876, 877, 878, 879, 880, 881, 883, 884, 885, 887, 888, 889, 890, 891, 892, 893, 895, 896, 898, 899, 902, 903, 904, 906, 907, 909, 910, 911, 913, 914, 916, 918, 919, 920, 921, 922, 923, 925, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 938, 941, 942, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 956, 958, 961, 962, 963, 965, 967, 969, 970, 971, 973, 974, 975, 977, 979, 980, 981, 983, 984, 985, 986, 988, 990, 992, 993, 994, 996, 997, 999, 1000, 1001, 1002, 1005, 1006, 1007, 1009, 1012, 1013, 1018, 1019, 1022, 1024, 1026, 1027, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1038, 1039, 1040, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1061, 1062, 1065, 1066, 1068, 1069, 1070, 1073, 1074, 1075, 1076, 1079, 1080, 1081, 1082, 1084, 1087, 1091, 1092, 1094, 1096, 1098, 1100, 1101, 1103, 1105, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1118, 1120, 1121, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1135, 1136, 1138, 1140, 1141, 1142, 1143, 1144, 1149, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1172, 1173, 1174, 1176, 1177, 1181, 1183, 1184, 1185, 1186, 1188, 1190, 1194, 1195, 1197, 1198, 1200, 1203, 1206, 1207, 1208, 1209, 1211, 1215, 1216, 1217, 1218, 1221, 1222, 1223, 1225, 1229, 1230, 1231, 1233, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1249, 1250, 1251, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294, 1298, 1300, 1302, 1303, 1304, 1306, 1307, 1308, 1309, 1310, 1313, 1314, 1315, 1317, 1318, 1319, 1321, 1322, 1323, 1326, 1327, 1329, 1330, 1331, 1332, 1333, 1335, 1337, 1339, 1340, 1343, 1344, 1345, 1346, 1348, 1349, 1351, 1352, 1354, 1356, 1357, 1358, 1359, 1360, 1364, 1365, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1380, 1381, 1382, 1383, 1385, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1411, 1412, 1414, 1415, 1416, 1417, 1421, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1462, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1476, 1478, 1479, 1480, 1481, 1484, 1485, 1486, 1487, 1488, 1490, 1491, 1493, 1495, 1498, 1500, 1501, 1502, 1503, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1517, 1518, 1519, 1520, 1523, 1525, 1526, 1527, 1530, 1531, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1544, 1546, 1547, 1549, 1550, 1551, 1552, 1554, 1556, 1557, 1559, 1560, 1561, 1562, 1564, 1568, 1569, 1575, 1576, 1577, 1579, 1580, 1582, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1609, 1610, 1611, 1613, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1627, 1628, 1629, 1630, 1631, 1632, 1635, 1636, 1638, 1639, 1641, 1642, 1643, 1644, 1645, 1646, 1648, 1650, 1651, 1652, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1664, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1703, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1713, 1714, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1745, 1746, 1747, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1766, 1767, 1768, 1770, 1771, 1772, 1774, 1776, 1777, 1778, 1779, 1780, 1781, 1783, 1784, 1785, 1786, 1788, 1789, 1790, 1791, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1802, 1803, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1830, 1832, 1833, 1834, 1835, 1836, 1837, 1839, 1841, 1842, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1870, 1871, 1872, 1873, 1874, 1877, 1878, 1879, 1880, 1882, 1887, 1889, 1892, 1893, 1894, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1905, 1906, 1907, 1908, 1910, 1911, 1912, 1913, 1915, 1916, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1927, 1928, 1929, 1930, 1931, 1932, 1935, 1936, 1938, 1941, 1943, 1944, 1946, 1947, 1948, 1949, 1951, 1952, 1954, 1955, 1956, 1957, 1961, 1963, 1965, 1966, 1967, 1968, 1969, 1972, 1975, 1976, 1978, 1980, 1981, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1995, 1996, 1998, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2025, 2026, 2027, 2029, 2031, 2032, 2033, 2035, 2036, 2037, 2038, 2040, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2051, 2052, 2053, 2054, 2055, 2058, 2059, 2060, 2061, 2062, 2064, 2065, 2067, 2068, 2071, 2074, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2084, 2085, 2086, 2088, 2089, 2091, 2092, 2094, 2095, 2096, 2098, 2099, 2100, 2102, 2103, 2104, 2105, 2106, 2109, 2110, 2111], 'partial_predict_sample': {'number': 0, 'ratio': 0.0, 'index': [], 'partial_correct_sample': {'number': 0, 'ratio': 0, 'index': []}, 'partial_notcorrect_sample': {'number': 0, 'ratio': 0, 'index': []}}, 'complete_predict_sample': {'number': 1503, 'ratio': 1.0, 'index': [1, 4, 5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 58, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 77, 78, 80, 81, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 117, 118, 120, 121, 122, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 157, 158, 159, 162, 163, 165, 167, 169, 170, 171, 172, 173, 175, 177, 178, 180, 181, 182, 184, 185, 186, 188, 190, 191, 193, 194, 195, 196, 199, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 219, 221, 222, 223, 225, 228, 231, 232, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 257, 259, 262, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 299, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 322, 325, 326, 327, 329, 330, 331, 333, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 352, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 377, 378, 379, 380, 381, 383, 385, 386, 387, 388, 389, 390, 391, 392, 394, 395, 397, 398, 400, 401, 402, 403, 404, 405, 406, 408, 410, 411, 412, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 427, 428, 429, 430, 432, 433, 434, 435, 438, 439, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 455, 457, 459, 460, 461, 462, 463, 466, 467, 469, 470, 472, 474, 475, 476, 477, 479, 480, 483, 484, 485, 487, 491, 492, 493, 494, 496, 498, 499, 500, 504, 505, 506, 508, 509, 510, 511, 512, 513, 514, 515, 517, 519, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 541, 542, 543, 544, 545, 546, 547, 550, 551, 552, 554, 555, 556, 557, 558, 559, 561, 562, 563, 565, 566, 569, 570, 571, 573, 574, 575, 577, 579, 580, 581, 582, 584, 585, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 620, 622, 623, 624, 625, 626, 627, 628, 630, 631, 632, 633, 634, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647, 648, 650, 652, 653, 654, 656, 657, 658, 659, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 678, 682, 683, 684, 685, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 711, 715, 717, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 740, 741, 742, 743, 745, 746, 747, 748, 750, 751, 752, 754, 755, 758, 759, 760, 761, 763, 765, 767, 769, 770, 771, 772, 773, 776, 777, 778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 793, 794, 795, 796, 797, 798, 799, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 818, 819, 820, 821, 823, 824, 825, 826, 827, 830, 831, 833, 834, 835, 836, 837, 838, 840, 841, 842, 843, 846, 847, 848, 850, 851, 852, 853, 854, 861, 863, 864, 865, 867, 868, 869, 871, 872, 874, 876, 877, 878, 879, 880, 881, 883, 884, 885, 887, 888, 889, 890, 891, 892, 893, 895, 896, 898, 899, 902, 903, 904, 906, 907, 909, 910, 911, 913, 914, 916, 918, 919, 920, 921, 922, 923, 925, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 938, 941, 942, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 956, 958, 961, 962, 963, 965, 967, 969, 970, 971, 973, 974, 975, 977, 979, 980, 981, 983, 984, 985, 986, 988, 990, 992, 993, 994, 996, 997, 999, 1000, 1001, 1002, 1005, 1006, 1007, 1009, 1012, 1013, 1018, 1019, 1022, 1024, 1026, 1027, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1038, 1039, 1040, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1061, 1062, 1065, 1066, 1068, 1069, 1070, 1073, 1074, 1075, 1076, 1079, 1080, 1081, 1082, 1084, 1087, 1091, 1092, 1094, 1096, 1098, 1100, 1101, 1103, 1105, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1118, 1120, 1121, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1135, 1136, 1138, 1140, 1141, 1142, 1143, 1144, 1149, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1172, 1173, 1174, 1176, 1177, 1181, 1183, 1184, 1185, 1186, 1188, 1190, 1194, 1195, 1197, 1198, 1200, 1203, 1206, 1207, 1208, 1209, 1211, 1215, 1216, 1217, 1218, 1221, 1222, 1223, 1225, 1229, 1230, 1231, 1233, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1249, 1250, 1251, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294, 1298, 1300, 1302, 1303, 1304, 1306, 1307, 1308, 1309, 1310, 1313, 1314, 1315, 1317, 1318, 1319, 1321, 1322, 1323, 1326, 1327, 1329, 1330, 1331, 1332, 1333, 1335, 1337, 1339, 1340, 1343, 1344, 1345, 1346, 1348, 1349, 1351, 1352, 1354, 1356, 1357, 1358, 1359, 1360, 1364, 1365, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1380, 1381, 1382, 1383, 1385, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1411, 1412, 1414, 1415, 1416, 1417, 1421, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1462, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1476, 1478, 1479, 1480, 1481, 1484, 1485, 1486, 1487, 1488, 1490, 1491, 1493, 1495, 1498, 1500, 1501, 1502, 1503, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1517, 1518, 1519, 1520, 1523, 1525, 1526, 1527, 1530, 1531, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1544, 1546, 1547, 1549, 1550, 1551, 1552, 1554, 1556, 1557, 1559, 1560, 1561, 1562, 1564, 1568, 1569, 1575, 1576, 1577, 1579, 1580, 1582, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1609, 1610, 1611, 1613, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1627, 1628, 1629, 1630, 1631, 1632, 1635, 1636, 1638, 1639, 1641, 1642, 1643, 1644, 1645, 1646, 1648, 1650, 1651, 1652, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1664, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1703, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1713, 1714, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1745, 1746, 1747, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1766, 1767, 1768, 1770, 1771, 1772, 1774, 1776, 1777, 1778, 1779, 1780, 1781, 1783, 1784, 1785, 1786, 1788, 1789, 1790, 1791, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1802, 1803, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1830, 1832, 1833, 1834, 1835, 1836, 1837, 1839, 1841, 1842, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1870, 1871, 1872, 1873, 1874, 1877, 1878, 1879, 1880, 1882, 1887, 1889, 1892, 1893, 1894, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1905, 1906, 1907, 1908, 1910, 1911, 1912, 1913, 1915, 1916, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1927, 1928, 1929, 1930, 1931, 1932, 1935, 1936, 1938, 1941, 1943, 1944, 1946, 1947, 1948, 1949, 1951, 1952, 1954, 1955, 1956, 1957, 1961, 1963, 1965, 1966, 1967, 1968, 1969, 1972, 1975, 1976, 1978, 1980, 1981, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1995, 1996, 1998, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2025, 2026, 2027, 2029, 2031, 2032, 2033, 2035, 2036, 2037, 2038, 2040, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2051, 2052, 2053, 2054, 2055, 2058, 2059, 2060, 2061, 2062, 2064, 2065, 2067, 2068, 2071, 2074, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2084, 2085, 2086, 2088, 2089, 2091, 2092, 2094, 2095, 2096, 2098, 2099, 2100, 2102, 2103, 2104, 2105, 2106, 2109, 2110, 2111], 'first_level_wrong_sample': {'number': 415, 'ratio': 0.2761144377910845, 'index': [4, 16, 27, 35, 36, 37, 43, 44, 45, 51, 58, 59, 60, 62, 64, 75, 78, 80, 86, 89, 92, 97, 99, 100, 104, 105, 107, 108, 117, 120, 121, 126, 127, 131, 134, 135, 140, 157, 159, 163, 165, 169, 171, 172, 180, 185, 194, 196, 202, 204, 205, 207, 210, 211, 212, 216, 221, 222, 225, 228, 231, 242, 250, 257, 259, 273, 276, 278, 281, 283, 287, 306, 314, 318, 325, 336, 339, 340, 346, 364, 366, 374, 378, 380, 383, 385, 387, 394, 395, 400, 402, 403, 410, 411, 412, 415, 416, 430, 443, 450, 452, 459, 466, 474, 480, 484, 485, 500, 505, 508, 510, 513, 521, 522, 524, 525, 526, 531, 532, 535, 543, 545, 552, 556, 558, 559, 561, 563, 569, 574, 582, 605, 609, 610, 620, 628, 631, 632, 641, 644, 658, 662, 669, 670, 671, 677, 683, 684, 691, 703, 707, 708, 717, 732, 736, 748, 751, 752, 755, 760, 763, 765, 769, 772, 777, 785, 787, 788, 794, 796, 805, 808, 814, 815, 819, 820, 821, 826, 836, 846, 847, 854, 864, 865, 867, 868, 869, 880, 884, 895, 910, 925, 931, 941, 944, 948, 949, 950, 952, 953, 961, 971, 975, 977, 986, 994, 1001, 1007, 1026, 1033, 1061, 1069, 1075, 1080, 1081, 1082, 1091, 1094, 1101, 1105, 1111, 1114, 1121, 1131, 1133, 1136, 1138, 1141, 1154, 1160, 1163, 1166, 1167, 1169, 1170, 1173, 1174, 1177, 1181, 1185, 1194, 1197, 1207, 1217, 1218, 1221, 1231, 1233, 1249, 1255, 1256, 1258, 1261, 1273, 1279, 1282, 1293, 1302, 1307, 1308, 1318, 1339, 1340, 1343, 1344, 1346, 1348, 1352, 1356, 1360, 1370, 1380, 1382, 1385, 1397, 1406, 1411, 1414, 1421, 1434, 1437, 1462, 1465, 1469, 1472, 1480, 1481, 1491, 1493, 1507, 1508, 1517, 1526, 1527, 1544, 1551, 1554, 1556, 1557, 1559, 1568, 1575, 1577, 1584, 1587, 1593, 1598, 1609, 1616, 1620, 1629, 1631, 1632, 1635, 1639, 1644, 1650, 1652, 1654, 1655, 1661, 1670, 1676, 1680, 1681, 1682, 1694, 1696, 1703, 1706, 1709, 1710, 1720, 1733, 1746, 1747, 1758, 1767, 1774, 1777, 1778, 1780, 1783, 1790, 1794, 1795, 1796, 1797, 1798, 1799, 1806, 1808, 1809, 1817, 1820, 1822, 1827, 1833, 1834, 1835, 1839, 1842, 1859, 1860, 1862, 1870, 1874, 1889, 1902, 1905, 1906, 1907, 1919, 1923, 1930, 1932, 1946, 1951, 1952, 1963, 1965, 1966, 1967, 1972, 1980, 1983, 1988, 1989, 1991, 2000, 2006, 2013, 2017, 2022, 2032, 2036, 2040, 2044, 2048, 2049, 2051, 2061, 2062, 2067, 2068, 2074, 2076, 2083, 2085, 2089, 2095, 2098, 2099, 2100, 2109]}, 'first_level_right_sample': {'number': 1088, 'ratio': 0.7238855622089155, 'index': [1, 5, 6, 8, 10, 12, 14, 15, 17, 19, 20, 21, 22, 23, 24, 26, 28, 29, 34, 38, 39, 41, 42, 46, 47, 48, 50, 52, 53, 54, 63, 65, 66, 68, 69, 70, 71, 72, 73, 77, 81, 84, 85, 88, 90, 93, 94, 95, 96, 98, 101, 103, 106, 109, 110, 113, 115, 118, 122, 124, 125, 128, 133, 136, 137, 138, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 158, 162, 167, 170, 173, 175, 177, 178, 181, 182, 184, 186, 188, 190, 191, 193, 195, 199, 201, 206, 209, 213, 215, 219, 223, 232, 234, 235, 236, 237, 238, 240, 241, 243, 244, 245, 247, 248, 251, 252, 253, 254, 262, 264, 269, 270, 274, 275, 277, 279, 282, 284, 285, 286, 288, 289, 291, 292, 293, 294, 295, 296, 299, 303, 304, 307, 308, 309, 310, 312, 313, 315, 316, 319, 322, 326, 327, 329, 330, 331, 333, 337, 338, 341, 342, 343, 345, 347, 348, 350, 352, 355, 356, 357, 358, 359, 360, 361, 362, 365, 367, 368, 369, 370, 372, 373, 375, 377, 379, 381, 386, 388, 389, 390, 391, 392, 397, 398, 401, 404, 405, 406, 408, 414, 417, 418, 420, 421, 422, 423, 424, 425, 427, 428, 429, 432, 433, 434, 435, 438, 439, 441, 442, 444, 445, 447, 448, 449, 451, 455, 457, 460, 461, 462, 463, 467, 469, 470, 472, 475, 476, 477, 479, 483, 487, 491, 492, 493, 494, 496, 498, 499, 504, 506, 509, 511, 512, 514, 515, 517, 519, 527, 528, 529, 530, 534, 536, 537, 541, 542, 544, 546, 547, 550, 551, 554, 555, 557, 562, 565, 566, 570, 571, 573, 575, 577, 579, 580, 581, 584, 585, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 607, 608, 611, 612, 613, 614, 616, 617, 622, 623, 624, 625, 626, 627, 630, 633, 634, 637, 638, 640, 642, 643, 645, 646, 647, 648, 650, 652, 653, 654, 656, 657, 659, 663, 664, 666, 667, 673, 674, 675, 676, 678, 682, 685, 686, 687, 689, 690, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 704, 705, 706, 709, 711, 715, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 733, 734, 735, 738, 740, 741, 742, 743, 745, 746, 747, 750, 754, 758, 759, 761, 767, 770, 771, 773, 776, 778, 779, 780, 782, 783, 784, 789, 790, 791, 793, 795, 797, 798, 799, 802, 803, 804, 806, 807, 809, 810, 811, 812, 813, 818, 823, 824, 825, 827, 830, 831, 833, 834, 835, 837, 838, 840, 841, 842, 843, 848, 850, 851, 852, 853, 861, 863, 871, 872, 874, 876, 877, 878, 879, 881, 883, 885, 887, 888, 889, 890, 891, 892, 893, 896, 898, 899, 902, 903, 904, 906, 907, 909, 911, 913, 914, 916, 918, 919, 920, 921, 922, 923, 927, 928, 929, 930, 932, 933, 934, 935, 936, 938, 942, 946, 947, 951, 954, 956, 958, 962, 963, 965, 967, 969, 970, 973, 974, 979, 980, 981, 983, 984, 985, 988, 990, 992, 993, 996, 997, 999, 1000, 1002, 1005, 1006, 1009, 1012, 1013, 1018, 1019, 1022, 1024, 1027, 1030, 1031, 1032, 1034, 1035, 1037, 1038, 1039, 1040, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1062, 1065, 1066, 1068, 1070, 1073, 1074, 1076, 1079, 1084, 1087, 1092, 1096, 1098, 1100, 1103, 1108, 1109, 1110, 1112, 1113, 1115, 1117, 1118, 1120, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1135, 1140, 1142, 1143, 1144, 1149, 1153, 1155, 1156, 1157, 1158, 1159, 1161, 1162, 1164, 1165, 1168, 1172, 1176, 1183, 1184, 1186, 1188, 1190, 1195, 1198, 1200, 1203, 1206, 1208, 1209, 1211, 1215, 1216, 1222, 1223, 1225, 1229, 1230, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1250, 1251, 1253, 1254, 1257, 1259, 1260, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1274, 1275, 1276, 1277, 1278, 1280, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1294, 1298, 1300, 1303, 1304, 1306, 1309, 1310, 1313, 1314, 1315, 1317, 1319, 1321, 1322, 1323, 1326, 1327, 1329, 1330, 1331, 1332, 1333, 1335, 1337, 1345, 1349, 1351, 1354, 1357, 1358, 1359, 1364, 1365, 1366, 1367, 1368, 1371, 1373, 1374, 1375, 1381, 1383, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1412, 1415, 1416, 1417, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1435, 1436, 1439, 1440, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1466, 1467, 1468, 1470, 1471, 1473, 1476, 1478, 1479, 1484, 1485, 1486, 1487, 1488, 1490, 1495, 1498, 1500, 1501, 1502, 1503, 1505, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1518, 1519, 1520, 1523, 1525, 1530, 1531, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1546, 1547, 1549, 1550, 1552, 1560, 1561, 1562, 1564, 1569, 1576, 1579, 1580, 1582, 1585, 1586, 1588, 1589, 1590, 1591, 1594, 1595, 1596, 1597, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1610, 1611, 1613, 1615, 1617, 1618, 1619, 1621, 1622, 1623, 1624, 1625, 1627, 1628, 1630, 1636, 1638, 1641, 1642, 1643, 1645, 1646, 1648, 1651, 1656, 1657, 1658, 1659, 1660, 1662, 1664, 1666, 1667, 1668, 1669, 1671, 1672, 1675, 1677, 1678, 1679, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1695, 1697, 1698, 1700, 1701, 1705, 1707, 1708, 1711, 1713, 1714, 1716, 1717, 1718, 1719, 1721, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1745, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1759, 1760, 1761, 1762, 1763, 1764, 1766, 1768, 1770, 1771, 1772, 1776, 1779, 1781, 1784, 1785, 1786, 1788, 1789, 1791, 1793, 1802, 1803, 1804, 1805, 1810, 1811, 1815, 1816, 1818, 1819, 1821, 1823, 1824, 1825, 1826, 1828, 1830, 1832, 1836, 1837, 1841, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1861, 1863, 1864, 1865, 1867, 1868, 1871, 1872, 1873, 1877, 1878, 1879, 1880, 1882, 1887, 1892, 1893, 1894, 1896, 1897, 1898, 1899, 1900, 1901, 1908, 1910, 1911, 1912, 1913, 1915, 1916, 1918, 1920, 1921, 1922, 1924, 1927, 1928, 1929, 1931, 1935, 1936, 1938, 1941, 1943, 1944, 1947, 1948, 1949, 1954, 1955, 1956, 1957, 1961, 1968, 1969, 1975, 1976, 1978, 1981, 1984, 1985, 1986, 1987, 1990, 1995, 1996, 1998, 2002, 2003, 2004, 2005, 2007, 2011, 2012, 2014, 2015, 2016, 2018, 2019, 2020, 2021, 2025, 2026, 2027, 2029, 2031, 2033, 2035, 2037, 2038, 2043, 2045, 2046, 2047, 2052, 2053, 2054, 2055, 2058, 2059, 2060, 2064, 2065, 2071, 2075, 2077, 2078, 2080, 2082, 2084, 2086, 2088, 2091, 2092, 2094, 2096, 2102, 2103, 2104, 2105, 2106, 2110, 2111], 'wrong_under_infection_sample': {'number': 906, 'ratio': 0.8327205882352942, 'index': [1, 5, 8, 10, 12, 14, 15, 17, 19, 20, 22, 23, 24, 28, 29, 38, 41, 42, 46, 48, 50, 52, 53, 54, 63, 65, 66, 68, 69, 70, 71, 72, 73, 77, 88, 90, 94, 96, 98, 101, 103, 106, 113, 115, 118, 124, 125, 128, 133, 136, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 158, 162, 170, 173, 175, 177, 181, 182, 186, 188, 190, 191, 193, 195, 199, 201, 206, 209, 215, 219, 223, 232, 234, 235, 236, 237, 238, 240, 248, 251, 252, 253, 254, 264, 269, 274, 277, 279, 282, 284, 285, 286, 291, 292, 293, 294, 295, 296, 299, 303, 304, 307, 308, 309, 310, 312, 313, 315, 319, 326, 327, 330, 331, 333, 337, 338, 341, 342, 347, 348, 350, 356, 359, 360, 362, 365, 367, 368, 370, 372, 375, 377, 379, 381, 386, 388, 389, 390, 391, 392, 397, 398, 401, 404, 405, 408, 414, 418, 420, 421, 422, 424, 427, 428, 429, 432, 433, 434, 435, 438, 439, 441, 442, 444, 445, 447, 448, 449, 451, 455, 457, 460, 461, 463, 467, 469, 470, 476, 477, 479, 483, 487, 492, 493, 494, 496, 498, 499, 504, 506, 511, 512, 514, 519, 527, 528, 529, 534, 537, 541, 542, 544, 546, 547, 550, 551, 554, 555, 557, 562, 565, 566, 570, 573, 575, 577, 579, 580, 581, 584, 585, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 607, 611, 612, 613, 614, 616, 617, 622, 623, 624, 625, 626, 627, 630, 633, 637, 638, 640, 642, 643, 645, 646, 648, 650, 652, 653, 654, 656, 657, 659, 663, 664, 666, 667, 673, 674, 675, 676, 678, 682, 685, 687, 689, 690, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 705, 706, 709, 711, 715, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 734, 735, 738, 740, 741, 742, 743, 746, 747, 750, 754, 758, 759, 761, 767, 770, 771, 773, 776, 778, 779, 780, 782, 783, 784, 789, 791, 793, 795, 797, 798, 799, 802, 804, 806, 807, 809, 810, 811, 812, 813, 824, 825, 827, 830, 831, 833, 834, 835, 838, 840, 841, 842, 843, 848, 850, 851, 852, 853, 861, 863, 872, 874, 876, 877, 878, 879, 881, 883, 885, 887, 889, 890, 892, 893, 896, 898, 899, 902, 903, 904, 906, 907, 911, 913, 916, 918, 919, 920, 921, 922, 923, 927, 928, 929, 930, 932, 933, 934, 935, 936, 938, 942, 947, 951, 954, 956, 958, 962, 963, 965, 969, 970, 973, 979, 980, 981, 984, 985, 990, 992, 993, 996, 997, 999, 1002, 1005, 1009, 1012, 1013, 1019, 1022, 1024, 1027, 1031, 1032, 1034, 1035, 1037, 1039, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1062, 1065, 1066, 1068, 1070, 1073, 1074, 1076, 1079, 1084, 1087, 1092, 1098, 1100, 1103, 1108, 1109, 1110, 1112, 1113, 1122, 1123, 1126, 1127, 1129, 1130, 1140, 1142, 1144, 1149, 1153, 1155, 1156, 1157, 1159, 1162, 1164, 1172, 1176, 1183, 1184, 1186, 1190, 1195, 1198, 1200, 1203, 1206, 1208, 1211, 1215, 1216, 1222, 1223, 1225, 1229, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1250, 1251, 1253, 1254, 1257, 1260, 1265, 1266, 1267, 1269, 1270, 1274, 1275, 1276, 1277, 1278, 1280, 1286, 1287, 1288, 1291, 1292, 1298, 1300, 1304, 1306, 1313, 1314, 1315, 1319, 1322, 1326, 1327, 1329, 1331, 1332, 1335, 1337, 1345, 1349, 1351, 1357, 1358, 1359, 1365, 1366, 1368, 1371, 1373, 1374, 1375, 1381, 1387, 1388, 1390, 1393, 1394, 1396, 1399, 1400, 1401, 1403, 1404, 1405, 1412, 1416, 1417, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1435, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1466, 1467, 1468, 1471, 1473, 1476, 1478, 1479, 1484, 1485, 1487, 1488, 1490, 1495, 1498, 1501, 1502, 1503, 1505, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1518, 1519, 1520, 1523, 1530, 1531, 1534, 1535, 1536, 1537, 1539, 1541, 1542, 1547, 1549, 1552, 1560, 1561, 1562, 1564, 1569, 1576, 1579, 1582, 1585, 1586, 1589, 1590, 1594, 1595, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1610, 1613, 1615, 1618, 1619, 1621, 1622, 1623, 1625, 1627, 1628, 1630, 1638, 1641, 1643, 1645, 1646, 1648, 1651, 1656, 1657, 1659, 1662, 1664, 1666, 1667, 1669, 1671, 1675, 1677, 1678, 1679, 1684, 1685, 1686, 1688, 1689, 1690, 1695, 1697, 1698, 1700, 1701, 1705, 1708, 1711, 1713, 1717, 1718, 1719, 1721, 1722, 1725, 1728, 1729, 1730, 1731, 1732, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1749, 1750, 1751, 1752, 1753, 1754, 1759, 1762, 1763, 1764, 1766, 1768, 1770, 1771, 1772, 1776, 1779, 1781, 1784, 1785, 1786, 1788, 1789, 1791, 1793, 1802, 1803, 1804, 1805, 1810, 1811, 1818, 1819, 1821, 1823, 1824, 1825, 1826, 1828, 1830, 1832, 1836, 1837, 1841, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1861, 1863, 1864, 1865, 1867, 1868, 1872, 1873, 1878, 1879, 1880, 1882, 1887, 1892, 1893, 1894, 1896, 1898, 1900, 1901, 1908, 1911, 1912, 1913, 1915, 1916, 1918, 1921, 1922, 1924, 1927, 1928, 1929, 1931, 1935, 1936, 1938, 1941, 1943, 1944, 1947, 1948, 1949, 1954, 1955, 1956, 1957, 1961, 1968, 1978, 1981, 1984, 1985, 1986, 1987, 1990, 1995, 1996, 1998, 2002, 2003, 2004, 2007, 2011, 2014, 2015, 2016, 2021, 2025, 2027, 2031, 2035, 2037, 2038, 2043, 2045, 2046, 2047, 2052, 2053, 2054, 2055, 2058, 2060, 2064, 2065, 2071, 2075, 2077, 2078, 2082, 2084, 2086, 2088, 2094, 2096, 2102, 2103, 2104, 2105, 2106, 2110, 2111]}, 'wrong_under_noninfection_sample': {'number': 182, 'ratio': 0.16727941176470587, 'index': [1538, 515, 517, 6, 1030, 1546, 2059, 1038, 1550, 1040, 530, 1042, 21, 536, 26, 2080, 34, 39, 2091, 2092, 1580, 47, 1588, 1591, 571, 1596, 1597, 1599, 1096, 1611, 589, 81, 1617, 84, 85, 1624, 1115, 93, 1117, 95, 608, 1118, 1120, 1124, 1636, 1642, 109, 110, 1135, 1143, 634, 122, 1658, 1660, 1668, 1158, 647, 1672, 137, 138, 1161, 1165, 1168, 1691, 1188, 167, 1707, 686, 178, 1714, 1716, 184, 1209, 1723, 704, 1230, 1745, 213, 733, 1757, 1760, 1761, 745, 1259, 1262, 241, 243, 244, 245, 1268, 247, 1271, 1272, 1284, 1285, 262, 270, 1294, 275, 790, 1303, 1815, 1816, 1309, 1310, 288, 289, 803, 1317, 1321, 1323, 818, 1330, 1333, 823, 316, 322, 837, 329, 1354, 1871, 1364, 1877, 343, 1367, 345, 352, 355, 357, 358, 871, 1383, 361, 1897, 1899, 369, 1395, 373, 1398, 1910, 888, 1402, 891, 1920, 1415, 909, 914, 406, 1436, 1439, 1440, 417, 423, 425, 1969, 946, 1975, 1976, 1470, 967, 462, 974, 1486, 2005, 983, 472, 475, 988, 1500, 2012, 2018, 2019, 2020, 1000, 2026, 491, 2029, 1006, 2033, 1525, 1018, 509], 'wrong_under_NIID_and_Neo_sample': {'number': 57, 'ratio': 0.3131868131868132, 'index': [517, 6, 1030, 1040, 2080, 2091, 2092, 1597, 1624, 93, 1120, 1636, 109, 1135, 634, 137, 138, 1165, 1691, 1714, 184, 213, 1760, 1262, 241, 262, 270, 1317, 1323, 818, 1330, 316, 322, 1354, 1871, 358, 369, 373, 891, 914, 406, 1440, 423, 425, 974, 1486, 983, 1500, 2019, 2020, 2026, 491, 2029, 2033, 1525, 1018, 509]}, 'wrong_at_NIID_and_Neo_sample': {'number': 125, 'ratio': 0.6868131868131868, 'index': [1538, 515, 1546, 2059, 1038, 1550, 530, 1042, 21, 536, 26, 34, 39, 1580, 47, 1588, 1591, 571, 1596, 1599, 1096, 1611, 589, 81, 1617, 84, 85, 1115, 1117, 95, 608, 1118, 1124, 1642, 110, 1143, 122, 1658, 1660, 1668, 1158, 647, 1672, 1161, 1168, 1188, 167, 1707, 686, 178, 1716, 1209, 1723, 704, 1230, 1745, 733, 1757, 1761, 745, 1259, 243, 244, 245, 1268, 247, 1271, 1272, 1284, 1285, 1294, 275, 790, 1303, 1815, 1816, 1309, 1310, 288, 289, 803, 1321, 1333, 823, 837, 329, 1364, 1877, 343, 1367, 345, 352, 355, 357, 871, 1383, 361, 1897, 1899, 1395, 1398, 1910, 888, 1402, 1920, 1415, 909, 1436, 1439, 417, 1969, 946, 1975, 1976, 1470, 967, 462, 2005, 472, 475, 988, 2012, 2018, 1000, 1006]}}, 'wrong_with_hierarchy_error_sample': {'number': 0, 'ratio': 0.0, 'index': []}}}}}, 'LCA': {'test_sample_size': {'number': 2112, 'label_target': array([[1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.]]), 'label_pred': array([[1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]])}, 'exact_match_sample': {'number': 609, 'ratio': 0.2883522727272727, 'index': [0, 2, 3, 7, 9, 11, 13, 18, 25, 30, 31, 32, 33, 40, 49, 55, 56, 57, 61, 67, 74, 76, 79, 82, 83, 87, 91, 102, 111, 112, 114, 116, 119, 123, 129, 130, 132, 139, 142, 147, 148, 150, 151, 156, 160, 161, 164, 166, 168, 174, 176, 179, 183, 187, 189, 192, 197, 198, 200, 203, 208, 214, 217, 218, 220, 224, 226, 227, 229, 230, 233, 239, 246, 249, 255, 256, 258, 260, 261, 263, 265, 266, 267, 268, 271, 272, 280, 290, 297, 298, 300, 301, 302, 305, 311, 317, 320, 321, 323, 324, 328, 332, 334, 335, 344, 349, 351, 353, 354, 363, 371, 376, 382, 384, 393, 396, 399, 407, 409, 413, 419, 426, 431, 436, 437, 440, 446, 453, 454, 456, 458, 464, 465, 468, 471, 473, 478, 481, 482, 486, 488, 489, 490, 495, 497, 501, 502, 503, 507, 516, 518, 520, 523, 533, 538, 539, 540, 548, 549, 553, 560, 564, 567, 568, 572, 576, 578, 583, 586, 587, 588, 591, 596, 599, 606, 615, 618, 619, 621, 629, 635, 636, 639, 649, 651, 655, 660, 661, 665, 668, 672, 679, 680, 681, 688, 699, 710, 712, 713, 714, 716, 718, 721, 727, 737, 739, 744, 749, 753, 756, 757, 762, 764, 766, 768, 774, 775, 781, 786, 792, 800, 801, 816, 817, 822, 828, 829, 832, 839, 844, 845, 849, 855, 856, 857, 858, 859, 860, 862, 866, 870, 873, 875, 882, 886, 894, 897, 900, 901, 905, 908, 912, 915, 917, 924, 926, 937, 939, 940, 943, 945, 955, 957, 959, 960, 964, 966, 968, 972, 976, 978, 982, 987, 989, 991, 995, 998, 1003, 1004, 1008, 1010, 1011, 1014, 1015, 1016, 1017, 1020, 1021, 1023, 1025, 1028, 1029, 1036, 1041, 1043, 1047, 1050, 1053, 1057, 1060, 1063, 1064, 1067, 1071, 1072, 1077, 1078, 1083, 1085, 1086, 1088, 1089, 1090, 1093, 1095, 1097, 1099, 1102, 1104, 1106, 1107, 1116, 1119, 1125, 1128, 1132, 1134, 1137, 1139, 1145, 1146, 1147, 1148, 1150, 1151, 1152, 1171, 1175, 1178, 1179, 1180, 1182, 1187, 1189, 1191, 1192, 1193, 1196, 1199, 1201, 1202, 1204, 1205, 1210, 1212, 1213, 1214, 1219, 1220, 1224, 1226, 1227, 1228, 1232, 1235, 1236, 1238, 1240, 1242, 1243, 1244, 1246, 1252, 1263, 1264, 1281, 1283, 1289, 1290, 1295, 1296, 1297, 1299, 1301, 1305, 1311, 1312, 1316, 1320, 1324, 1325, 1328, 1334, 1336, 1338, 1341, 1342, 1347, 1350, 1353, 1355, 1361, 1362, 1363, 1369, 1372, 1376, 1377, 1378, 1379, 1384, 1386, 1389, 1391, 1392, 1407, 1408, 1409, 1410, 1413, 1418, 1419, 1420, 1423, 1426, 1427, 1429, 1432, 1438, 1441, 1442, 1447, 1448, 1455, 1459, 1463, 1464, 1474, 1475, 1477, 1482, 1483, 1489, 1492, 1494, 1496, 1497, 1499, 1504, 1506, 1516, 1521, 1522, 1524, 1528, 1529, 1532, 1533, 1540, 1543, 1545, 1548, 1553, 1555, 1558, 1563, 1565, 1566, 1567, 1570, 1571, 1572, 1573, 1574, 1578, 1581, 1583, 1592, 1601, 1608, 1612, 1614, 1626, 1633, 1634, 1637, 1640, 1647, 1649, 1653, 1663, 1665, 1673, 1674, 1683, 1687, 1692, 1693, 1699, 1702, 1704, 1712, 1715, 1724, 1726, 1727, 1737, 1743, 1744, 1748, 1755, 1756, 1765, 1769, 1773, 1775, 1782, 1787, 1792, 1800, 1801, 1807, 1812, 1813, 1814, 1829, 1831, 1838, 1840, 1844, 1850, 1853, 1855, 1866, 1869, 1875, 1876, 1881, 1883, 1884, 1885, 1886, 1888, 1890, 1891, 1895, 1903, 1904, 1909, 1914, 1917, 1925, 1926, 1933, 1934, 1937, 1939, 1940, 1942, 1945, 1950, 1953, 1958, 1959, 1960, 1962, 1964, 1970, 1971, 1973, 1974, 1977, 1979, 1982, 1992, 1993, 1994, 1997, 1999, 2001, 2008, 2009, 2010, 2023, 2024, 2028, 2030, 2034, 2039, 2041, 2042, 2050, 2056, 2057, 2063, 2066, 2069, 2070, 2072, 2073, 2079, 2081, 2087, 2090, 2093, 2097, 2101, 2107, 2108]}, 'not_exact_match_sample': {'number': 1503, 'ratio': 0.7116477272727273, 'index': [1, 4, 5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 58, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 77, 78, 80, 81, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 117, 118, 120, 121, 122, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 157, 158, 159, 162, 163, 165, 167, 169, 170, 171, 172, 173, 175, 177, 178, 180, 181, 182, 184, 185, 186, 188, 190, 191, 193, 194, 195, 196, 199, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 219, 221, 222, 223, 225, 228, 231, 232, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 257, 259, 262, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 299, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 322, 325, 326, 327, 329, 330, 331, 333, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 352, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 377, 378, 379, 380, 381, 383, 385, 386, 387, 388, 389, 390, 391, 392, 394, 395, 397, 398, 400, 401, 402, 403, 404, 405, 406, 408, 410, 411, 412, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 427, 428, 429, 430, 432, 433, 434, 435, 438, 439, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 455, 457, 459, 460, 461, 462, 463, 466, 467, 469, 470, 472, 474, 475, 476, 477, 479, 480, 483, 484, 485, 487, 491, 492, 493, 494, 496, 498, 499, 500, 504, 505, 506, 508, 509, 510, 511, 512, 513, 514, 515, 517, 519, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 541, 542, 543, 544, 545, 546, 547, 550, 551, 552, 554, 555, 556, 557, 558, 559, 561, 562, 563, 565, 566, 569, 570, 571, 573, 574, 575, 577, 579, 580, 581, 582, 584, 585, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 620, 622, 623, 624, 625, 626, 627, 628, 630, 631, 632, 633, 634, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647, 648, 650, 652, 653, 654, 656, 657, 658, 659, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 678, 682, 683, 684, 685, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 711, 715, 717, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 740, 741, 742, 743, 745, 746, 747, 748, 750, 751, 752, 754, 755, 758, 759, 760, 761, 763, 765, 767, 769, 770, 771, 772, 773, 776, 777, 778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 793, 794, 795, 796, 797, 798, 799, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 818, 819, 820, 821, 823, 824, 825, 826, 827, 830, 831, 833, 834, 835, 836, 837, 838, 840, 841, 842, 843, 846, 847, 848, 850, 851, 852, 853, 854, 861, 863, 864, 865, 867, 868, 869, 871, 872, 874, 876, 877, 878, 879, 880, 881, 883, 884, 885, 887, 888, 889, 890, 891, 892, 893, 895, 896, 898, 899, 902, 903, 904, 906, 907, 909, 910, 911, 913, 914, 916, 918, 919, 920, 921, 922, 923, 925, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 938, 941, 942, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 956, 958, 961, 962, 963, 965, 967, 969, 970, 971, 973, 974, 975, 977, 979, 980, 981, 983, 984, 985, 986, 988, 990, 992, 993, 994, 996, 997, 999, 1000, 1001, 1002, 1005, 1006, 1007, 1009, 1012, 1013, 1018, 1019, 1022, 1024, 1026, 1027, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1038, 1039, 1040, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1061, 1062, 1065, 1066, 1068, 1069, 1070, 1073, 1074, 1075, 1076, 1079, 1080, 1081, 1082, 1084, 1087, 1091, 1092, 1094, 1096, 1098, 1100, 1101, 1103, 1105, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1118, 1120, 1121, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1135, 1136, 1138, 1140, 1141, 1142, 1143, 1144, 1149, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1172, 1173, 1174, 1176, 1177, 1181, 1183, 1184, 1185, 1186, 1188, 1190, 1194, 1195, 1197, 1198, 1200, 1203, 1206, 1207, 1208, 1209, 1211, 1215, 1216, 1217, 1218, 1221, 1222, 1223, 1225, 1229, 1230, 1231, 1233, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1249, 1250, 1251, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294, 1298, 1300, 1302, 1303, 1304, 1306, 1307, 1308, 1309, 1310, 1313, 1314, 1315, 1317, 1318, 1319, 1321, 1322, 1323, 1326, 1327, 1329, 1330, 1331, 1332, 1333, 1335, 1337, 1339, 1340, 1343, 1344, 1345, 1346, 1348, 1349, 1351, 1352, 1354, 1356, 1357, 1358, 1359, 1360, 1364, 1365, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1380, 1381, 1382, 1383, 1385, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1411, 1412, 1414, 1415, 1416, 1417, 1421, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1462, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1476, 1478, 1479, 1480, 1481, 1484, 1485, 1486, 1487, 1488, 1490, 1491, 1493, 1495, 1498, 1500, 1501, 1502, 1503, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1517, 1518, 1519, 1520, 1523, 1525, 1526, 1527, 1530, 1531, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1544, 1546, 1547, 1549, 1550, 1551, 1552, 1554, 1556, 1557, 1559, 1560, 1561, 1562, 1564, 1568, 1569, 1575, 1576, 1577, 1579, 1580, 1582, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1609, 1610, 1611, 1613, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1627, 1628, 1629, 1630, 1631, 1632, 1635, 1636, 1638, 1639, 1641, 1642, 1643, 1644, 1645, 1646, 1648, 1650, 1651, 1652, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1664, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1703, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1713, 1714, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1745, 1746, 1747, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1766, 1767, 1768, 1770, 1771, 1772, 1774, 1776, 1777, 1778, 1779, 1780, 1781, 1783, 1784, 1785, 1786, 1788, 1789, 1790, 1791, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1802, 1803, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1830, 1832, 1833, 1834, 1835, 1836, 1837, 1839, 1841, 1842, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1870, 1871, 1872, 1873, 1874, 1877, 1878, 1879, 1880, 1882, 1887, 1889, 1892, 1893, 1894, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1905, 1906, 1907, 1908, 1910, 1911, 1912, 1913, 1915, 1916, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1927, 1928, 1929, 1930, 1931, 1932, 1935, 1936, 1938, 1941, 1943, 1944, 1946, 1947, 1948, 1949, 1951, 1952, 1954, 1955, 1956, 1957, 1961, 1963, 1965, 1966, 1967, 1968, 1969, 1972, 1975, 1976, 1978, 1980, 1981, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1995, 1996, 1998, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2025, 2026, 2027, 2029, 2031, 2032, 2033, 2035, 2036, 2037, 2038, 2040, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2051, 2052, 2053, 2054, 2055, 2058, 2059, 2060, 2061, 2062, 2064, 2065, 2067, 2068, 2071, 2074, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2084, 2085, 2086, 2088, 2089, 2091, 2092, 2094, 2095, 2096, 2098, 2099, 2100, 2102, 2103, 2104, 2105, 2106, 2109, 2110, 2111], 'partial_path_sample': {'number': 0, 'ratio': 0.0, 'index': [], 'partial_correct_sample': {'number': 0, 'ratio': 0, 'index': []}}, 'complete_path_sample': {'number': 1503, 'ratio': 0.7116477272727273, 'index': [1, 4, 5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 58, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 77, 78, 80, 81, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 117, 118, 120, 121, 122, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 157, 158, 159, 162, 163, 165, 167, 169, 170, 171, 172, 173, 175, 177, 178, 180, 181, 182, 184, 185, 186, 188, 190, 191, 193, 194, 195, 196, 199, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 219, 221, 222, 223, 225, 228, 231, 232, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 257, 259, 262, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 299, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 322, 325, 326, 327, 329, 330, 331, 333, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 352, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 377, 378, 379, 380, 381, 383, 385, 386, 387, 388, 389, 390, 391, 392, 394, 395, 397, 398, 400, 401, 402, 403, 404, 405, 406, 408, 410, 411, 412, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 427, 428, 429, 430, 432, 433, 434, 435, 438, 439, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 455, 457, 459, 460, 461, 462, 463, 466, 467, 469, 470, 472, 474, 475, 476, 477, 479, 480, 483, 484, 485, 487, 491, 492, 493, 494, 496, 498, 499, 500, 504, 505, 506, 508, 509, 510, 511, 512, 513, 514, 515, 517, 519, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 541, 542, 543, 544, 545, 546, 547, 550, 551, 552, 554, 555, 556, 557, 558, 559, 561, 562, 563, 565, 566, 569, 570, 571, 573, 574, 575, 577, 579, 580, 581, 582, 584, 585, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 620, 622, 623, 624, 625, 626, 627, 628, 630, 631, 632, 633, 634, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647, 648, 650, 652, 653, 654, 656, 657, 658, 659, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 678, 682, 683, 684, 685, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 711, 715, 717, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 740, 741, 742, 743, 745, 746, 747, 748, 750, 751, 752, 754, 755, 758, 759, 760, 761, 763, 765, 767, 769, 770, 771, 772, 773, 776, 777, 778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 793, 794, 795, 796, 797, 798, 799, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 818, 819, 820, 821, 823, 824, 825, 826, 827, 830, 831, 833, 834, 835, 836, 837, 838, 840, 841, 842, 843, 846, 847, 848, 850, 851, 852, 853, 854, 861, 863, 864, 865, 867, 868, 869, 871, 872, 874, 876, 877, 878, 879, 880, 881, 883, 884, 885, 887, 888, 889, 890, 891, 892, 893, 895, 896, 898, 899, 902, 903, 904, 906, 907, 909, 910, 911, 913, 914, 916, 918, 919, 920, 921, 922, 923, 925, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 938, 941, 942, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 956, 958, 961, 962, 963, 965, 967, 969, 970, 971, 973, 974, 975, 977, 979, 980, 981, 983, 984, 985, 986, 988, 990, 992, 993, 994, 996, 997, 999, 1000, 1001, 1002, 1005, 1006, 1007, 1009, 1012, 1013, 1018, 1019, 1022, 1024, 1026, 1027, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1038, 1039, 1040, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1061, 1062, 1065, 1066, 1068, 1069, 1070, 1073, 1074, 1075, 1076, 1079, 1080, 1081, 1082, 1084, 1087, 1091, 1092, 1094, 1096, 1098, 1100, 1101, 1103, 1105, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1118, 1120, 1121, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1135, 1136, 1138, 1140, 1141, 1142, 1143, 1144, 1149, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1172, 1173, 1174, 1176, 1177, 1181, 1183, 1184, 1185, 1186, 1188, 1190, 1194, 1195, 1197, 1198, 1200, 1203, 1206, 1207, 1208, 1209, 1211, 1215, 1216, 1217, 1218, 1221, 1222, 1223, 1225, 1229, 1230, 1231, 1233, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1249, 1250, 1251, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294, 1298, 1300, 1302, 1303, 1304, 1306, 1307, 1308, 1309, 1310, 1313, 1314, 1315, 1317, 1318, 1319, 1321, 1322, 1323, 1326, 1327, 1329, 1330, 1331, 1332, 1333, 1335, 1337, 1339, 1340, 1343, 1344, 1345, 1346, 1348, 1349, 1351, 1352, 1354, 1356, 1357, 1358, 1359, 1360, 1364, 1365, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1380, 1381, 1382, 1383, 1385, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1411, 1412, 1414, 1415, 1416, 1417, 1421, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1462, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1476, 1478, 1479, 1480, 1481, 1484, 1485, 1486, 1487, 1488, 1490, 1491, 1493, 1495, 1498, 1500, 1501, 1502, 1503, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1517, 1518, 1519, 1520, 1523, 1525, 1526, 1527, 1530, 1531, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1544, 1546, 1547, 1549, 1550, 1551, 1552, 1554, 1556, 1557, 1559, 1560, 1561, 1562, 1564, 1568, 1569, 1575, 1576, 1577, 1579, 1580, 1582, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1609, 1610, 1611, 1613, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1627, 1628, 1629, 1630, 1631, 1632, 1635, 1636, 1638, 1639, 1641, 1642, 1643, 1644, 1645, 1646, 1648, 1650, 1651, 1652, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1664, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1703, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1713, 1714, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1745, 1746, 1747, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1766, 1767, 1768, 1770, 1771, 1772, 1774, 1776, 1777, 1778, 1779, 1780, 1781, 1783, 1784, 1785, 1786, 1788, 1789, 1790, 1791, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1802, 1803, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1830, 1832, 1833, 1834, 1835, 1836, 1837, 1839, 1841, 1842, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1870, 1871, 1872, 1873, 1874, 1877, 1878, 1879, 1880, 1882, 1887, 1889, 1892, 1893, 1894, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1905, 1906, 1907, 1908, 1910, 1911, 1912, 1913, 1915, 1916, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1927, 1928, 1929, 1930, 1931, 1932, 1935, 1936, 1938, 1941, 1943, 1944, 1946, 1947, 1948, 1949, 1951, 1952, 1954, 1955, 1956, 1957, 1961, 1963, 1965, 1966, 1967, 1968, 1969, 1972, 1975, 1976, 1978, 1980, 1981, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1995, 1996, 1998, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2025, 2026, 2027, 2029, 2031, 2032, 2033, 2035, 2036, 2037, 2038, 2040, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2051, 2052, 2053, 2054, 2055, 2058, 2059, 2060, 2061, 2062, 2064, 2065, 2067, 2068, 2071, 2074, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2084, 2085, 2086, 2088, 2089, 2091, 2092, 2094, 2095, 2096, 2098, 2099, 2100, 2102, 2103, 2104, 2105, 2106, 2109, 2110, 2111], 'notcorrect_predict_path_sample': {'number': 0, 'ratio': 0.0, 'index': []}, 'correct_predict_path_sample': {'number': 1503, 'ratio': 0.7116477272727273, 'index': [1, 4, 5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 58, 59, 60, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 77, 78, 80, 81, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 117, 118, 120, 121, 122, 124, 125, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 140, 141, 143, 144, 145, 146, 149, 152, 153, 154, 155, 157, 158, 159, 162, 163, 165, 167, 169, 170, 171, 172, 173, 175, 177, 178, 180, 181, 182, 184, 185, 186, 188, 190, 191, 193, 194, 195, 196, 199, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 213, 215, 216, 219, 221, 222, 223, 225, 228, 231, 232, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 245, 247, 248, 250, 251, 252, 253, 254, 257, 259, 262, 264, 269, 270, 273, 274, 275, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 299, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 318, 319, 322, 325, 326, 327, 329, 330, 331, 333, 336, 337, 338, 339, 340, 341, 342, 343, 345, 346, 347, 348, 350, 352, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 372, 373, 374, 375, 377, 378, 379, 380, 381, 383, 385, 386, 387, 388, 389, 390, 391, 392, 394, 395, 397, 398, 400, 401, 402, 403, 404, 405, 406, 408, 410, 411, 412, 414, 415, 416, 417, 418, 420, 421, 422, 423, 424, 425, 427, 428, 429, 430, 432, 433, 434, 435, 438, 439, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 455, 457, 459, 460, 461, 462, 463, 466, 467, 469, 470, 472, 474, 475, 476, 477, 479, 480, 483, 484, 485, 487, 491, 492, 493, 494, 496, 498, 499, 500, 504, 505, 506, 508, 509, 510, 511, 512, 513, 514, 515, 517, 519, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 541, 542, 543, 544, 545, 546, 547, 550, 551, 552, 554, 555, 556, 557, 558, 559, 561, 562, 563, 565, 566, 569, 570, 571, 573, 574, 575, 577, 579, 580, 581, 582, 584, 585, 589, 590, 592, 593, 594, 595, 597, 598, 600, 601, 602, 603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 620, 622, 623, 624, 625, 626, 627, 628, 630, 631, 632, 633, 634, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647, 648, 650, 652, 653, 654, 656, 657, 658, 659, 662, 663, 664, 666, 667, 669, 670, 671, 673, 674, 675, 676, 677, 678, 682, 683, 684, 685, 686, 687, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 711, 715, 717, 719, 720, 722, 723, 724, 725, 726, 728, 729, 730, 731, 732, 733, 734, 735, 736, 738, 740, 741, 742, 743, 745, 746, 747, 748, 750, 751, 752, 754, 755, 758, 759, 760, 761, 763, 765, 767, 769, 770, 771, 772, 773, 776, 777, 778, 779, 780, 782, 783, 784, 785, 787, 788, 789, 790, 791, 793, 794, 795, 796, 797, 798, 799, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 818, 819, 820, 821, 823, 824, 825, 826, 827, 830, 831, 833, 834, 835, 836, 837, 838, 840, 841, 842, 843, 846, 847, 848, 850, 851, 852, 853, 854, 861, 863, 864, 865, 867, 868, 869, 871, 872, 874, 876, 877, 878, 879, 880, 881, 883, 884, 885, 887, 888, 889, 890, 891, 892, 893, 895, 896, 898, 899, 902, 903, 904, 906, 907, 909, 910, 911, 913, 914, 916, 918, 919, 920, 921, 922, 923, 925, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 938, 941, 942, 944, 946, 947, 948, 949, 950, 951, 952, 953, 954, 956, 958, 961, 962, 963, 965, 967, 969, 970, 971, 973, 974, 975, 977, 979, 980, 981, 983, 984, 985, 986, 988, 990, 992, 993, 994, 996, 997, 999, 1000, 1001, 1002, 1005, 1006, 1007, 1009, 1012, 1013, 1018, 1019, 1022, 1024, 1026, 1027, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1038, 1039, 1040, 1042, 1044, 1045, 1046, 1048, 1049, 1051, 1052, 1054, 1055, 1056, 1058, 1059, 1061, 1062, 1065, 1066, 1068, 1069, 1070, 1073, 1074, 1075, 1076, 1079, 1080, 1081, 1082, 1084, 1087, 1091, 1092, 1094, 1096, 1098, 1100, 1101, 1103, 1105, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1117, 1118, 1120, 1121, 1122, 1123, 1124, 1126, 1127, 1129, 1130, 1131, 1133, 1135, 1136, 1138, 1140, 1141, 1142, 1143, 1144, 1149, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1172, 1173, 1174, 1176, 1177, 1181, 1183, 1184, 1185, 1186, 1188, 1190, 1194, 1195, 1197, 1198, 1200, 1203, 1206, 1207, 1208, 1209, 1211, 1215, 1216, 1217, 1218, 1221, 1222, 1223, 1225, 1229, 1230, 1231, 1233, 1234, 1237, 1239, 1241, 1245, 1247, 1248, 1249, 1250, 1251, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1284, 1285, 1286, 1287, 1288, 1291, 1292, 1293, 1294, 1298, 1300, 1302, 1303, 1304, 1306, 1307, 1308, 1309, 1310, 1313, 1314, 1315, 1317, 1318, 1319, 1321, 1322, 1323, 1326, 1327, 1329, 1330, 1331, 1332, 1333, 1335, 1337, 1339, 1340, 1343, 1344, 1345, 1346, 1348, 1349, 1351, 1352, 1354, 1356, 1357, 1358, 1359, 1360, 1364, 1365, 1366, 1367, 1368, 1370, 1371, 1373, 1374, 1375, 1380, 1381, 1382, 1383, 1385, 1387, 1388, 1390, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1411, 1412, 1414, 1415, 1416, 1417, 1421, 1422, 1424, 1425, 1428, 1430, 1431, 1433, 1434, 1435, 1436, 1437, 1439, 1440, 1443, 1444, 1445, 1446, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1460, 1461, 1462, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1476, 1478, 1479, 1480, 1481, 1484, 1485, 1486, 1487, 1488, 1490, 1491, 1493, 1495, 1498, 1500, 1501, 1502, 1503, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1517, 1518, 1519, 1520, 1523, 1525, 1526, 1527, 1530, 1531, 1534, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1544, 1546, 1547, 1549, 1550, 1551, 1552, 1554, 1556, 1557, 1559, 1560, 1561, 1562, 1564, 1568, 1569, 1575, 1576, 1577, 1579, 1580, 1582, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1609, 1610, 1611, 1613, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1627, 1628, 1629, 1630, 1631, 1632, 1635, 1636, 1638, 1639, 1641, 1642, 1643, 1644, 1645, 1646, 1648, 1650, 1651, 1652, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1664, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1703, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1713, 1714, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1738, 1739, 1740, 1741, 1742, 1745, 1746, 1747, 1749, 1750, 1751, 1752, 1753, 1754, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1766, 1767, 1768, 1770, 1771, 1772, 1774, 1776, 1777, 1778, 1779, 1780, 1781, 1783, 1784, 1785, 1786, 1788, 1789, 1790, 1791, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1802, 1803, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1830, 1832, 1833, 1834, 1835, 1836, 1837, 1839, 1841, 1842, 1843, 1845, 1846, 1847, 1848, 1849, 1851, 1852, 1854, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1870, 1871, 1872, 1873, 1874, 1877, 1878, 1879, 1880, 1882, 1887, 1889, 1892, 1893, 1894, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1905, 1906, 1907, 1908, 1910, 1911, 1912, 1913, 1915, 1916, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1927, 1928, 1929, 1930, 1931, 1932, 1935, 1936, 1938, 1941, 1943, 1944, 1946, 1947, 1948, 1949, 1951, 1952, 1954, 1955, 1956, 1957, 1961, 1963, 1965, 1966, 1967, 1968, 1969, 1972, 1975, 1976, 1978, 1980, 1981, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1995, 1996, 1998, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2025, 2026, 2027, 2029, 2031, 2032, 2033, 2035, 2036, 2037, 2038, 2040, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2051, 2052, 2053, 2054, 2055, 2058, 2059, 2060, 2061, 2062, 2064, 2065, 2067, 2068, 2071, 2074, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2084, 2085, 2086, 2088, 2089, 2091, 2092, 2094, 2095, 2096, 2098, 2099, 2100, 2102, 2103, 2104, 2105, 2106, 2109, 2110, 2111], 'lca_height': 1.635395874916833, 'lca_heights': [1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 3, 3, 3, 1, 2, 1, 1, 3, 3, 3, 1, 2, 1, 1, 3, 1, 1, 1, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 2, 2, 2, 3, 1, 3, 1, 3, 1, 1, 2, 1, 3, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 1, 2, 1, 1, 3, 1, 3, 3, 2, 1, 1, 3, 3, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 3, 2, 3, 1, 3, 3, 1, 1, 1, 2, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 2, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 2, 3, 1, 3, 1, 3, 1, 3, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 2, 2, 3, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 2, 1, 3, 1, 1, 1, 2, 3, 2, 1, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 2, 3, 3, 1, 3, 2, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 3, 3, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 1, 3, 3, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 2, 1, 1, 3, 1, 3, 1, 1, 1, 1, 2, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 2, 1, 3, 3, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 3, 2, 1, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3, 2, 2, 2, 1, 3, 1, 1, 2, 1, 1, 1, 1, 3, 3, 1, 3, 3, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 3, 2, 1, 3, 1, 1, 3, 3, 2, 3, 3, 1, 3, 3, 1, 3, 3, 1, 1, 3, 1, 2, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 1, 3, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 1, 3, 2, 2, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 2, 1, 1, 3, 3, 2, 2, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 3, 1, 3, 2, 3, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 3, 3, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 2, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 3, 1, 3, 3, 3, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 2, 1, 3, 1, 1, 3, 2, 1, 1, 2, 3, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 1, 3, 1, 2, 1, 3, 1, 1, 1, 3, 1, 3, 3, 3, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3, 2, 1, 3, 3, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 2, 2, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 3, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 2, 3, 2, 2, 1, 3, 1, 3, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 3, 3, 1, 1, 3, 3, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1]}}}}, 'error_details': ['609(28.8352%)', '1503(71.1648%)', '0(0.0000%)', '0(0.0000%)', '0(0.0000%)', '1503(100.0000%)', '415(27.6114%)', '1088(72.3886%)', '906(83.2721%)', '182(16.7279%)', '125(68.6813%)', '57(31.3187%)', '0(0.0000%)'], 'lca_details': ['2112', '609(28.8352%)', '1503(71.1648%)', '0(0.0000%)', '0(0.0000%)', '1503(71.1648%)', '0(0.0000%)', '1503(71.1648%)', '1.635395874916833'], 'target_labels': array([[1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.]]), 'pred_labels': array([[1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [1., 0., 1., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate4test(config_0, target_labels_array, predcit_labels_array)\n",
    "\n",
    "# save the pred and true labels of TEST dataset\n",
    "metrics['target_labels'] = target_labels_array\n",
    "metrics['pred_labels'] = predcit_labels_array\n",
    "\n",
    "np.save(os.path.join(\"./results/hp_tuning/PreAttnMMs_LCPN\", \"test_results.npy\"), metrics)\n",
    "print(\"The process finished!\")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('hie_attn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "4e1aa0fb0bd10bf5277393d95b1f7e9c2d03411dc295f291055dba03e839c981"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "502007a8cd46d72d7be7ff31ad2d0b486cf121fd9c67c11a479532d3153e2a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

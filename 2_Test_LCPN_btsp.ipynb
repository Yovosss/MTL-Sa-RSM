{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Test Prediction Metrics for PreAttnMMs_LCPN\n",
    "        - do some necessary post-processing for PreAttnMMs_LCPN model results\n",
    "        - at last, we will get the target_labels and pred_labels of TEST dataset with shape [sample_num, 11]\n",
    "        - we will calculate some metrics based on the above target_labels and pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from addict import Dict\n",
    "from sklearn.metrics import (accuracy_score, auc, average_precision_score,\n",
    "                             classification_report, confusion_matrix, f1_score,\n",
    "                             hamming_loss, precision_recall_curve,\n",
    "                             precision_score, recall_score, roc_auc_score,\n",
    "                             roc_curve, zero_one_loss)\n",
    "from torch import nn\n",
    "\n",
    "from helper.data import DataPreprocess\n",
    "from helper.data_loader import data_loaders\n",
    "from helper.utils import get_config, load_checkpoint, set_seed\n",
    "from models.model import PreAttnMMs\n",
    "from train_modules.evaluation_metrics import evaluate4test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the model structure and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  Loading previously preprocessed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wzx/HC4FUOV2/models/grud_layer.py:120: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/data/wzx/HC4FUOV2/models/grud_layer.py:120: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/zju/anaconda3/envs/hie_attn/lib/python3.6/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "config_0 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_0\"))\n",
    "config_1 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_1\"))\n",
    "config_2 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_2\"))\n",
    "config_6 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_6\"))\n",
    "config_7 = Dict(get_config(config_id=\"PreAttnMMs_LCPN_7\"))\n",
    "\n",
    "set_seed(seed=config_0.seed)\n",
    "\n",
    "# Load the preprocessed data\n",
    "dp = DataPreprocess(config_0)\n",
    "data, label, indices = dp.load()\n",
    "\n",
    "n_classes_0 = len(label['taxonomy'][config_0.experiment.local_task])\n",
    "n_classes_1 = len(label['taxonomy'][config_1.experiment.local_task])\n",
    "n_classes_2 = len(label['taxonomy'][config_2.experiment.local_task])\n",
    "n_classes_6 = len(label['taxonomy'][config_6.experiment.local_task])\n",
    "n_classes_7 = len(label['taxonomy'][config_7.experiment.local_task])\n",
    "\n",
    "model_0 = PreAttnMMs(config_0, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_0)\n",
    "model_0.to(config_0.train.device_setting.device)\n",
    "model_1 = PreAttnMMs(config_1, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_1)\n",
    "model_1.to(config_1.train.device_setting.device)\n",
    "model_2 = PreAttnMMs(config_2, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_2)\n",
    "model_2.to(config_2.train.device_setting.device)\n",
    "model_6 = PreAttnMMs(config_6, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_6)\n",
    "model_6.to(config_6.train.device_setting.device)\n",
    "model_7 = PreAttnMMs(config_7, \n",
    "                   data['X_t_steps'], \n",
    "                   data['X_t_features'],\n",
    "                   data['X_features'],\n",
    "                   n_classes_7)\n",
    "model_7.to(config_7.train.device_setting.device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer_0 = torch.optim.Adam(\n",
    "    params = model_0.parameters(),\n",
    "    lr = config_0.train.optimizer.learning_rate,\n",
    "    weight_decay=config_0.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_1 = torch.optim.AdamW(\n",
    "    params = model_1.parameters(),\n",
    "    lr = config_1.train.optimizer.learning_rate,\n",
    "    weight_decay=config_1.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_2 = torch.optim.RMSprop(\n",
    "    params = model_2.parameters(),\n",
    "    lr = config_2.train.optimizer.learning_rate,\n",
    "    weight_decay=config_2.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_6 = torch.optim.RMSprop(\n",
    "    params = model_6.parameters(),\n",
    "    lr = config_6.train.optimizer.learning_rate,\n",
    "    weight_decay=config_6.train.optimizer.weight_decay\n",
    ")\n",
    "optimizer_7 = torch.optim.Adagrad(\n",
    "    params = model_7.parameters(),\n",
    "    lr = config_7.train.optimizer.learning_rate,\n",
    "    weight_decay=config_7.train.optimizer.weight_decay\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read the checkpoint file and load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial90).pt for node-0 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial65).pt for node-1 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial94).pt for node-2 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial77).pt for node-6 task\n",
      "Loading the checkpoint --> best_best_checkpoint(i.e.trial21).pt for node-7 task\n"
     ]
    }
   ],
   "source": [
    "checkpoint_base = config_0.train.checkpoint.dir\n",
    "checkpoint_dir_0 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-0', 'Standardization', 'auc')\n",
    "checkpoint_dir_1 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-1', 'Standardization', 'macro-auc')\n",
    "checkpoint_dir_2 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-2', 'Standardization', 'auc')\n",
    "checkpoint_dir_6 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-6', 'Standardization', 'auc')\n",
    "checkpoint_dir_7 = os.path.join(checkpoint_base, 'hp_tuning', 'PreAttnMMs_LCPN', 'node-7', 'Standardization', 'auc')\n",
    "\n",
    "# get the best checkpoint .pt file\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_0):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_0 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_0, 0))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_1):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_1 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_1, 1))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_2):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_2 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_2, 2))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_6):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_6 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_6, 6))\n",
    "\n",
    "checkpoints = []\n",
    "idx_checkpoint = np.array([])\n",
    "for i in os.listdir(checkpoint_dir_7):\n",
    "    if \"best_best_checkpoint\" in i:\n",
    "        checkpoints.append(i)\n",
    "for i in checkpoints:\n",
    "    seachobj = re.search(r\"\\d+(?=\\).pt)\", i)\n",
    "    idx_checkpoint = np.append(idx_checkpoint, int(seachobj.group()))\n",
    "target_model_7 = checkpoints[np.argmax(idx_checkpoint)]\n",
    "\n",
    "print(\"Loading the checkpoint --> {} for node-{} task\".format(target_model_7, 7))\n",
    "\n",
    "\n",
    "# reload the checkpoint file and run on test Dataset\n",
    "best_epoch_model_file_0 = os.path.join(checkpoint_dir_0, target_model_0)\n",
    "if os.path.isfile(best_epoch_model_file_0):\n",
    "    best_performance_0, config_0 = load_checkpoint(best_epoch_model_file_0, \n",
    "                                                model=model_0,\n",
    "                                                config=config_0,\n",
    "                                                optimizer=optimizer_0)\n",
    "best_epoch_model_file_1 = os.path.join(checkpoint_dir_1, target_model_1)\n",
    "if os.path.isfile(best_epoch_model_file_1):\n",
    "    best_performance_1, config_1 = load_checkpoint(best_epoch_model_file_1, \n",
    "                                                model=model_1,\n",
    "                                                config=config_1,\n",
    "                                                optimizer=optimizer_1)\n",
    "best_epoch_model_file_2 = os.path.join(checkpoint_dir_2, target_model_2)\n",
    "if os.path.isfile(best_epoch_model_file_2):\n",
    "    best_performance_2, config_2 = load_checkpoint(best_epoch_model_file_2, \n",
    "                                                model=model_2,\n",
    "                                                config=config_2,\n",
    "                                                optimizer=optimizer_2)\n",
    "best_epoch_model_file_6 = os.path.join(checkpoint_dir_6, target_model_6)\n",
    "if os.path.isfile(best_epoch_model_file_6):\n",
    "    best_performance_6, config_6 = load_checkpoint(best_epoch_model_file_6, \n",
    "                                                model=model_6,\n",
    "                                                config=config_6,\n",
    "                                                optimizer=optimizer_6)\n",
    "best_epoch_model_file_7 = os.path.join(checkpoint_dir_7, target_model_7)\n",
    "if os.path.isfile(best_epoch_model_file_7):\n",
    "    best_performance_7, config_7 = load_checkpoint(best_epoch_model_file_7, \n",
    "                                                model=model_7,\n",
    "                                                config=config_7,\n",
    "                                                optimizer=optimizer_7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load the data and statistics of each local task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_0 = indices['folds_idx_with_txy']['parent-node-0'][0,:][2]\n",
    "indice_1 = indices['folds_idx_with_txy']['parent-node-1'][0,:][2]\n",
    "indice_2 = indices['folds_idx_with_txy']['parent-node-2'][0,:][2]\n",
    "indice_6 = indices['folds_idx_with_txy']['parent-node-6'][0,:][2]\n",
    "indice_7 = indices['folds_idx_with_txy']['parent-node-7'][0,:][2]\n",
    "sample_size = indice_0.shape[0]\n",
    "data_0 = {\n",
    "    'X_t': data['X_t'][indice_0], \n",
    "    'T_t': data['T_t_rel'][indice_0],\n",
    "    'X_t_mask': data['X_t_mask'][indice_0],\n",
    "    'deltaT_t': data['deltaT_t'][indice_0],\n",
    "    'X_val': data['static_data_val'][indice_0],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_0],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_0].tolist()\n",
    "}\n",
    "data_1 = {\n",
    "    'X_t': data['X_t'][indice_1], \n",
    "    'T_t': data['T_t_rel'][indice_1],\n",
    "    'X_t_mask': data['X_t_mask'][indice_1],\n",
    "    'deltaT_t': data['deltaT_t'][indice_1],\n",
    "    'X_val': data['static_data_val'][indice_1],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_1],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_1].tolist()\n",
    "}\n",
    "data_2 = {\n",
    "    'X_t': data['X_t'][indice_2], \n",
    "    'T_t': data['T_t_rel'][indice_2],\n",
    "    'X_t_mask': data['X_t_mask'][indice_2],\n",
    "    'deltaT_t': data['deltaT_t'][indice_2],\n",
    "    'X_val': data['static_data_val'][indice_2],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_2],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_2].tolist()\n",
    "}\n",
    "data_6 = {\n",
    "    'X_t': data['X_t'][indice_6], \n",
    "    'T_t': data['T_t_rel'][indice_6],\n",
    "    'X_t_mask': data['X_t_mask'][indice_6],\n",
    "    'deltaT_t': data['deltaT_t'][indice_6],\n",
    "    'X_val': data['static_data_val'][indice_6],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_6],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_6].tolist()\n",
    "}\n",
    "data_7 = {\n",
    "    'X_t': data['X_t'][indice_7], \n",
    "    'T_t': data['T_t_rel'][indice_7],\n",
    "    'X_t_mask': data['X_t_mask'][indice_7],\n",
    "    'deltaT_t': data['deltaT_t'][indice_7],\n",
    "    'X_val': data['static_data_val'][indice_7],\n",
    "    'X_cat': data['static_data_cat_onehot'][indice_7],\n",
    "    'y_classes_unique': np.array(label['y_classes_unique'], dtype=object)[indice_7].tolist()\n",
    "}\n",
    "\n",
    "stat_0 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_0.experiment.local_task)][config_0.data.kfold][1,3]\n",
    "}\n",
    "stat_1 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_1.experiment.local_task)][config_1.data.kfold][1,3]\n",
    "}\n",
    "stat_2 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_2.experiment.local_task)][config_2.data.kfold][1,3]\n",
    "}\n",
    "stat_6 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_6.experiment.local_task)][config_6.data.kfold][1,3]\n",
    "}\n",
    "stat_7 = {\n",
    "    'X_val_mean': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,0],\n",
    "    'X_val_std': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,1],\n",
    "    'X_val_max': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,2],\n",
    "    'X_val_min': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][0,3],\n",
    "    'X_t_mean': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,0],\n",
    "    'X_t_std': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,1],\n",
    "    'X_t_max': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,2],\n",
    "    'X_t_min': indices['folds_stats']['parent-node-{}'.format(config_7.experiment.local_task)][config_7.data.kfold][1,3]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rescale_stdize(x, mean, std):\n",
    "    \"\"\"\n",
    "    standardize the non-time-series data and time-series data\n",
    "    :param x: A np.array witn shape (t_i, d)\n",
    "            mean: A np.array with shape (d,)\n",
    "            std: A np.array with shape (d,)\n",
    "    :return A np.array with same shape as x with rescaled values\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        return (x - mean) / std\n",
    "    elif x.ndim == 2:\n",
    "        return (x - mean[np.newaxis, :]) / std[np.newaxis, :]\n",
    "    elif x.ndim == 3:\n",
    "        return np.asarray([(xx - mean[np.newaxis, :]) / std[np.newaxis, :] for xx in x])\n",
    "\n",
    "def _fillnan(x, mean):\n",
    "        \"\"\"\n",
    "        fill the nan value in non-time-series data\n",
    "        :param x: A np.array of static variables with shape (d,)\n",
    "               mean: A np.array of mean value of each variable with shape (d,)\n",
    "        :return A np.array without nan value\n",
    "        \"\"\"\n",
    "        x[np.isnan(x)] = mean[np.isnan(x)]\n",
    "\n",
    "        return x  \n",
    "\n",
    "def _rescale_stdize(x, mean, std):\n",
    "    \"\"\"\n",
    "    standardize the non-time-series data and time-series data\n",
    "    :param x: A np.array witn shape (t_i, d)\n",
    "            mean: A np.array with shape (d,)\n",
    "            std: A np.array with shape (d,)\n",
    "    :return A np.array with same shape as x with rescaled values\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        return (x - mean) / std\n",
    "    elif x.ndim == 2:\n",
    "        return (x - mean[np.newaxis, :]) / std[np.newaxis, :]\n",
    "    elif x.ndim == 3:\n",
    "        return np.asarray([(xx - mean[np.newaxis, :]) / std[np.newaxis, :] for xx in x])\n",
    "\n",
    "def _locf_numpy(X, X_nan):\n",
    "    \"\"\"Numpy implementation of LOCF.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray,\n",
    "        Time series containing missing values (NaN) to be imputed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_imputed : array,\n",
    "        Imputed time series.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This implementation gets inspired by the question on StackOverflow:\n",
    "    https://stackoverflow.com/questions/41190852/most-efficient-way-to-forward-fill-nan-values-in-numpy-array\n",
    "    \"\"\"\n",
    "    trans_X = X.transpose((1, 0))\n",
    "    trans_X_nan = X_nan.transpose((1, 0))\n",
    "    mask = np.isnan(trans_X_nan)\n",
    "    n_features, n_steps  = mask.shape\n",
    "    idx = np.where(~mask, np.arange(n_steps), 0)\n",
    "    np.maximum.accumulate(idx, axis=1, out=idx)\n",
    "\n",
    "    X_imputed = trans_X[np.arange(n_features)[:, None], idx]\n",
    "    X_imputed = X_imputed.transpose((1, 0))\n",
    "\n",
    "    # If there are values still missing,\n",
    "    # they are missing at the beginning of the time-series sequence.\n",
    "    # Impute them with self.nan\n",
    "    if np.isnan(X_imputed).any():\n",
    "        X_imputed = np.nan_to_num(X_imputed, nan=0)\n",
    "\n",
    "    return X_imputed\n",
    "\n",
    "def f_empirical_mean(data, stat):\n",
    "\n",
    "    X_rescaled = _rescale_stdize(data['X_t'], stat['X_t_mean'], stat['X_t_std'])\n",
    "    X_rescaled = X_rescaled.reshape(-1, data['X_t'].shape[-1])\n",
    "    empirical_mean = np.nanmean(X_rescaled, axis=0)\n",
    "\n",
    "    return empirical_mean\n",
    "\n",
    "def _preprocess_sample(raw_sample, stat, empirical_mean):\n",
    "    \"\"\"\"\n",
    "    normalize the sample data\n",
    "    :param: raw_sample -> List[array(), array(), array(), array(), List[List[]]]\n",
    "    :return: Dict{'X_t': np.array([]),\n",
    "                    'X': np.array([]),\n",
    "                    'X_t_mask': np.array([]),\n",
    "                    'deltaT_t': np.array([]),\n",
    "                    'X_t_filledLOCF': np.array([]),\n",
    "                    'y_classes': List[List[int]],\n",
    "                    'empirical_mean': np.array([])}\n",
    "    \"\"\"\n",
    "    sample = {}\n",
    "\n",
    "    sample['X_t'] = _rescale_stdize(raw_sample[0], stat['X_t_mean'], stat['X_t_std'])\n",
    "    sample['X_t'] = np.nan_to_num(sample['X_t'])\n",
    "    \n",
    "    # fill the nan value in X_val and normalize X_val\n",
    "    raw_sample[4] = _fillnan(raw_sample[4], stat['X_val_mean'])\n",
    "    raw_sample[4] = _rescale_stdize(raw_sample[4], stat['X_val_mean'], stat['X_val_std'])\n",
    "    # concatenate the static variables\n",
    "    sample['X'] = np.concatenate((raw_sample[4], raw_sample[5]))\n",
    "\n",
    "    # forward fill nan value in np.array\n",
    "    sample['X_t_filledLOCF'] = _locf_numpy(sample['X_t'], raw_sample[0])\n",
    "\n",
    "    sample['empirical_mean'] = empirical_mean\n",
    "\n",
    "    sample['X_t_mask'] = raw_sample[2]\n",
    "    sample['deltaT_t'] = raw_sample[3] / 86400 # 24*60*60\n",
    "    sample['y_classes'] = raw_sample[-1]\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _localize_label(config, label, batch_labels):\n",
    "    \"\"\"\n",
    "    :param batch_labels: label idx of one batch, List[List[int]], e.g. [[0, 1, 3], [0, 2, 6, 9],...]\n",
    "    :return batch_local_label: np.array([int]), e.g. np.array([0, 1, ...])\n",
    "    \"\"\"\n",
    "    label_dict = {}\n",
    "    for idx, value in enumerate(label['taxonomy'][config.experiment.local_task]):\n",
    "        label_dict[value] = idx\n",
    "    \n",
    "    print(label_dict)\n",
    "    print(batch_labels)\n",
    "    \n",
    "    local_labels = []\n",
    "    for label in batch_labels:\n",
    "        for label_idx in label:\n",
    "            if label_idx in label_dict:\n",
    "                local_labels.append(label_dict[label_idx])\n",
    "                break\n",
    "\n",
    "    assert len(local_labels) == len(batch_labels), \"The labels are missed during localization, please recheck!\"\n",
    "\n",
    "    return local_labels\n",
    "\n",
    "def _check_input(batch):\n",
    "    for key, value in batch.items():\n",
    "        # convert the data type if in need\n",
    "        batch[key] = value.to(config_0.train.device_setting.device)\n",
    "\n",
    "    return batch\n",
    "\n",
    "def collect(config, label, batch):\n",
    "    batch_X_t = []\n",
    "    batch_X_t_mask = []\n",
    "    batch_deltaT_t = []\n",
    "    batch_X_t_filledLOCF = []\n",
    "    batch_empirical_mean = []\n",
    "    batch_X = []\n",
    "    batch_label = []\n",
    "\n",
    "    # copy batch_size times of one sample\n",
    "    batch_copy = []\n",
    "    for i in range(64):\n",
    "        batch_copy.append(batch)\n",
    "\n",
    "    for sample in batch_copy:\n",
    "        batch_X_t.append(sample['X_t'])\n",
    "        batch_X_t_mask.append(sample['X_t_mask'])\n",
    "        batch_deltaT_t.append(sample['deltaT_t'])\n",
    "        batch_X_t_filledLOCF.append(sample['X_t_filledLOCF'])\n",
    "        batch_empirical_mean.append(sample['empirical_mean'])\n",
    "        batch_X.append(sample['X'])\n",
    "        batch_label.append(sample['y_classes'])\n",
    "\n",
    "    return {\n",
    "        'X': torch.tensor(np.array(batch_X)).to(torch.float32),\n",
    "        'X_t': torch.tensor(np.array(batch_X_t)).to(torch.float32),\n",
    "        'X_t_mask': torch.tensor(np.array(batch_X_t_mask)).to(torch.float32),\n",
    "        'deltaT_t': torch.tensor(np.array(batch_deltaT_t)).to(torch.float32),\n",
    "        'X_t_filledLOCF': torch.tensor(np.array(batch_X_t_filledLOCF)).to(torch.float32),\n",
    "        'empirical_mean': torch.tensor(np.array(batch_empirical_mean)).to(torch.float32)\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make hierarchical predictions based on the label hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _all_node_label_wo_root(batch_label):\n",
    "    \"\"\"\n",
    "    tranform y_classes_unique to all-node label without ROOT node\n",
    "    :params: batch_label, List[List[], List[], ...]-->[[0,1,3], [0,1,5], [0,2,6,8], ...]\n",
    "    :Return: all_node_labels without root node, List[List[], List[], ...]-->[[1,0,1,0,0,0,0,0,0,0,0], [1,0,0,0,1,0,0,0,0,0,0],...]\n",
    "    \"\"\"\n",
    "    all_node_labels = np.zeros((len(batch_label), 11))\n",
    "    for i, label in enumerate(batch_label):\n",
    "        for j in label[1:]:\n",
    "            all_node_labels[i][j-1] = 1\n",
    "\n",
    "    return all_node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Test\n",
    "results_btsp = {}\n",
    "n_bootstrap = 1000\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    print(\"[Info] The {}-th iteration is processing!\".format(i))\n",
    "    idx = np.random.randint(0, len(indice_0), size=len(indice_0))\n",
    "    \n",
    "    target_labels_array = np.zeros((indice_0[:2112].shape[0], 11))\n",
    "    predcit_labels_array = np.zeros((indice_0[:2112].shape[0], 11))\n",
    "    \n",
    "    # resample labels and data\n",
    "    target_labels = data_0['y_classes_unique'][idx]\n",
    "    raw_sample = dict(map(lambda item: (item[0], item[1][idx]), data_0.items()))\n",
    "    \n",
    "    # transform the target labels of TEST dataset into form of [sample_size, 11]\n",
    "    target_labels_array = _all_node_label_wo_root(target_labels[:2112])\n",
    "\n",
    "    # test the sample one by one\n",
    "    for index, sample_idx in enumerate(indice_0[idx][:2112]):\n",
    "        raw_sample = [raw_sample[s][index] for s in ['X_t', 'T_t', 'X_t_mask', 'deltaT_t', 'X_val' , 'X_cat', 'y_classes_unique']]\n",
    "        \n",
    "        # parent-node-0\n",
    "        empirical_mean = f_empirical_mean(data_0, stat_0)\n",
    "        processed_sample = _preprocess_sample(raw_sample, stat_0, empirical_mean)\n",
    "        batch = collect(config_0, label, processed_sample)\n",
    "        \n",
    "        model_0.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = _check_input(batch)\n",
    "            logits = model_0(inputs)\n",
    "            predictions = F.softmax(logits, dim=1)\n",
    "            pred_label_0 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "            if pred_label_0 == 0:\n",
    "                predcit_labels_array[index, 0] = 1\n",
    "            elif pred_label_0 == 1:\n",
    "                predcit_labels_array[index, 1] = 1\n",
    "        \n",
    "        if pred_label_0 == 0:\n",
    "            # parent-node-1\n",
    "            empirical_mean = f_empirical_mean(data_1, stat_1)\n",
    "            processed_sample = _preprocess_sample(raw_sample, stat_1, empirical_mean)\n",
    "            batch = collect(config_1, label, processed_sample)\n",
    "\n",
    "            model_1.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs = _check_input(batch)\n",
    "                logits = model_1(inputs)\n",
    "                predictions = F.softmax(logits, dim=1)\n",
    "                pred_label_1 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "                if pred_label_1 == 0:\n",
    "                    predcit_labels_array[index, 2] = 1\n",
    "                elif pred_label_1 == 1:\n",
    "                    predcit_labels_array[index, 3] = 1\n",
    "                elif pred_label_1 == 2:\n",
    "                    predcit_labels_array[index, 4] = 1\n",
    "\n",
    "        elif pred_label_0 == 1:\n",
    "            # parent-node-2\n",
    "            empirical_mean = f_empirical_mean(data_2, stat_2)\n",
    "            processed_sample = _preprocess_sample(raw_sample, stat_2, empirical_mean)\n",
    "            batch = collect(config_2, label, processed_sample)\n",
    "\n",
    "            model_2.eval()\n",
    "            with torch.no_grad():\n",
    "                inputs = _check_input(batch)\n",
    "                logits = model_2(inputs)\n",
    "                predictions = F.softmax(logits, dim=1)\n",
    "                pred_label_2 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "                if pred_label_2 == 0:\n",
    "                    predcit_labels_array[index, 5] = 1\n",
    "                elif pred_label_2 == 1:\n",
    "                    predcit_labels_array[index, 6] = 1\n",
    "\n",
    "            if pred_label_2 == 0:\n",
    "                # parent-node-6\n",
    "                empirical_mean = f_empirical_mean(data_6, stat_6)\n",
    "                processed_sample = _preprocess_sample(raw_sample, stat_6, empirical_mean)\n",
    "                batch = collect(config_6, label, processed_sample)\n",
    "\n",
    "                model_6.eval()\n",
    "                with torch.no_grad():\n",
    "                    inputs = _check_input(batch)\n",
    "                    logits = model_6(inputs)\n",
    "                    predictions = F.softmax(logits, dim=1)\n",
    "                    pred_label_6 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "                    if pred_label_6 == 0:\n",
    "                        predcit_labels_array[index, 7] = 1\n",
    "                    elif pred_label_6 == 1:\n",
    "                        predcit_labels_array[index, 8] = 1\n",
    "        \n",
    "            elif pred_label_2 == 1:\n",
    "                # parent-node-7\n",
    "                empirical_mean = f_empirical_mean(data_7, stat_7)\n",
    "                processed_sample = _preprocess_sample(raw_sample, stat_7, empirical_mean)\n",
    "                batch = collect(config_7, label, processed_sample)\n",
    "\n",
    "                model_7.eval()\n",
    "                with torch.no_grad():\n",
    "                    inputs = _check_input(batch)\n",
    "                    logits = model_7(inputs)\n",
    "                    predictions = F.softmax(logits, dim=1)\n",
    "                    pred_label_7 = predictions.max(1)[-1].cpu().tolist()[0]\n",
    "                    if pred_label_7 == 0:\n",
    "                        predcit_labels_array[index, 9] = 1\n",
    "                    elif pred_label_7 == 1:\n",
    "                        predcit_labels_array[index, 10] = 1\n",
    "\n",
    "    metrics = evaluate4test(config_0, target_labels_array, predcit_labels_array)\n",
    "    # save the pred and true labels of TEST dataset\n",
    "    metrics['target_labels'] = target_labels_array\n",
    "    metrics['pred_labels'] = predcit_labels_array\n",
    "    \n",
    "    results_btsp[i] = metrics\n",
    "    \n",
    "np.save(os.path.join(\"./results/hp_tuning/PreAttnMMs_LCPN\", \"btsp_results.npy\"), metrics)\n",
    "print(\"The process finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('hie_attn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "4e1aa0fb0bd10bf5277393d95b1f7e9c2d03411dc295f291055dba03e839c981"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "502007a8cd46d72d7be7ff31ad2d0b486cf121fd9c67c11a479532d3153e2a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
